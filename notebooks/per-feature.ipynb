{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169ac227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the src folder to sys.path\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "from music_recommender.config import Config\n",
    "\n",
    "# Scikit-learn base classes and utilities\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "# Preprocessing & feature selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_absolute_error, mean_squared_error,\n",
    "    explained_variance_score, mean_absolute_percentage_error,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    balanced_accuracy_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedf364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\wsl.localhost\\Ubuntu-22.04\\home\\rime\\music-recom\\data\\processed\\audio \\\\wsl.localhost\\Ubuntu-22.04\\home\\rime\\music-recom\\data\\processed \\\\wsl.localhost\\Ubuntu-22.04\\home\\rime\\music-recom\\data\\interim\n"
     ]
    }
   ],
   "source": [
    "root = Path()\n",
    "\n",
    "cfg = Config()\n",
    "asp = cfg.paths.audio_spotify\n",
    "prc = cfg.paths.processed\n",
    "intr = cfg.paths.interim\n",
    "\n",
    "print(asp, prc, intr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37be063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLoader(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Load audio files from paths or bytes with caching support\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        sr: int = 22050,\n",
    "        use_cache: bool = True,\n",
    "        cache_dir: Optional[Union[str, Path]] = None,\n",
    "        max_cache_size_mb: int = 1000\n",
    "    ):\n",
    "        \n",
    "        self.sr = sr\n",
    "        self.use_cache = use_cache\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else Path('./audio_cache')\n",
    "        self.max_cache_size_mb = max_cache_size_mb\n",
    "        \n",
    "        # In-memory cache for current session\n",
    "        self._memory_cache = {}\n",
    "        \n",
    "        # Setup cache directory\n",
    "        if self.use_cache:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._cache_index_path = self.cache_dir / \"cache_index.pkl\"\n",
    "            self._load_cache_index()\n",
    "    \n",
    "    def _load_cache_index(self):\n",
    "        \"\"\"Load cache index (tracks what's cached and access times)\"\"\"\n",
    "        if self._cache_index_path.exists():\n",
    "            try:\n",
    "                with open(self._cache_index_path, 'rb') as f:\n",
    "                    self._cache_index = pickle.load(f)\n",
    "            except Exception:\n",
    "                self._cache_index = {}\n",
    "        else:\n",
    "            self._cache_index = {}\n",
    "    \n",
    "    def _save_cache_index(self):\n",
    "        \"\"\"Save cache index to disk\"\"\"\n",
    "        try:\n",
    "            with open(self._cache_index_path, 'wb') as f:\n",
    "                pickle.dump(self._cache_index, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save cache index: {e}\")\n",
    "    \n",
    "    def _get_cache_key(self, item: Union[str, Path, bytes]) -> str:\n",
    "        \"\"\"Generate unique cache key for an item\"\"\"\n",
    "        if isinstance(item, (str, Path)):\n",
    "            # For files: use path + modification time + sr\n",
    "            path = Path(item)\n",
    "            if path.exists():\n",
    "                mtime = path.stat().st_mtime\n",
    "                key_string = f\"{path.absolute()}_{mtime}_{self.sr}\"\n",
    "            else:\n",
    "                key_string = f\"{path.absolute()}_{self.sr}\"\n",
    "        elif isinstance(item, bytes):\n",
    "            # For bytes: use hash of content + sr\n",
    "            content_hash = hashlib.md5(item).hexdigest()\n",
    "            key_string = f\"bytes_{content_hash}_{self.sr}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input type: {type(item)}\")\n",
    "        \n",
    "        # Return hash of key_string\n",
    "        return hashlib.sha256(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve item from cache (memory or disk)\"\"\"\n",
    "        # Check memory cache first (fastest)\n",
    "        if cache_key in self._memory_cache:\n",
    "            return self._memory_cache[cache_key]\n",
    "        \n",
    "        # Check disk cache\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                # Update memory cache\n",
    "                self._memory_cache[cache_key] = data\n",
    "                \n",
    "                # Update access time in index\n",
    "                if cache_key in self._cache_index:\n",
    "                    self._cache_index[cache_key]['last_access'] = np.datetime64('now')\n",
    "                \n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load cache file {cache_file}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, cache_key: str, data: Dict[str, Any]):\n",
    "        \"\"\"Save item to cache (memory and disk)\"\"\"\n",
    "        # Save to memory cache\n",
    "        self._memory_cache[cache_key] = data\n",
    "        \n",
    "        # Save to disk cache\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            # Update cache index\n",
    "            file_size = cache_file.stat().st_size / (1024 * 1024)  # MB\n",
    "            self._cache_index[cache_key] = {\n",
    "                'file': cache_file,\n",
    "                'size_mb': file_size,\n",
    "                'last_access': np.datetime64('now'),\n",
    "                'created': np.datetime64('now')\n",
    "            }\n",
    "            \n",
    "            # Check cache size and cleanup if needed\n",
    "            self._cleanup_cache_if_needed()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save to cache: {e}\")\n",
    "    \n",
    "    def _cleanup_cache_if_needed(self):\n",
    "        \"\"\"Remove old cache entries if cache size exceeds limit\"\"\"\n",
    "        total_size = sum(info['size_mb'] for info in self._cache_index.values())\n",
    "        \n",
    "        if total_size > self.max_cache_size_mb:\n",
    "            # Sort by last access time (oldest first)\n",
    "            sorted_entries = sorted(\n",
    "                self._cache_index.items(),\n",
    "                key=lambda x: x[1]['last_access']\n",
    "            )\n",
    "            \n",
    "            # Remove oldest entries until under limit\n",
    "            for cache_key, info in sorted_entries:\n",
    "                if total_size <= self.max_cache_size_mb * 0.9:  # 90% of limit\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Remove file\n",
    "                    if info['file'].exists():\n",
    "                        info['file'].unlink()\n",
    "                    \n",
    "                    # Remove from index\n",
    "                    total_size -= info['size_mb']\n",
    "                    del self._cache_index[cache_key]\n",
    "                    \n",
    "                    # Remove from memory cache\n",
    "                    if cache_key in self._memory_cache:\n",
    "                        del self._memory_cache[cache_key]\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not remove cache file: {e}\")\n",
    "            \n",
    "            # Save updated index\n",
    "            self._save_cache_index()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform paths/bytes to audio data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : str, Path, bytes, or list of these\n",
    "            Audio file paths or raw audio bytes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of dictionaries containing audio data\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        if isinstance(X, (str, Path, bytes)):\n",
    "            X = [X]\n",
    "        elif hasattr(X, \"tolist\"):\n",
    "            X = X.tolist()\n",
    "        \n",
    "        results = []\n",
    "        cache_hits = 0\n",
    "        cache_misses = 0\n",
    "        \n",
    "        for item in tqdm(X, desc=\"Loading audio\"):\n",
    "            if self.use_cache:\n",
    "                cache_key = self._get_cache_key(item)\n",
    "                cached_data = self._get_from_cache(cache_key)\n",
    "                \n",
    "                if cached_data is not None:\n",
    "                    results.append(cached_data)\n",
    "                    cache_hits += 1\n",
    "                    continue\n",
    "            \n",
    "            # Load from source\n",
    "            loaded = self._load_single(item)\n",
    "            results.append(loaded)\n",
    "            cache_misses += 1\n",
    "            \n",
    "            # Save to cache\n",
    "            if self.use_cache:\n",
    "                self._save_to_cache(cache_key, loaded)\n",
    "        \n",
    "        # Print cache statistics\n",
    "        if self.use_cache and (cache_hits + cache_misses) > 0:\n",
    "            hit_rate = cache_hits / (cache_hits + cache_misses) * 100\n",
    "            print(f\"Cache: {cache_hits} hits, {cache_misses} misses ({hit_rate:.1f}% hit rate)\")\n",
    "        \n",
    "        return np.array(results, dtype=object)\n",
    "    \n",
    "    def _load_single(self, item: Union[str, Path, bytes]) -> Dict[str, Any]:\n",
    "        \"\"\"Load a single audio file\"\"\"\n",
    "        if isinstance(item, (str, Path)):\n",
    "            # Load from file path\n",
    "            audio, sr = librosa.load(item, sr=self.sr)\n",
    "            return {\n",
    "                \"audio\": audio,\n",
    "                \"sr\": sr,\n",
    "                \"path\": Path(item),\n",
    "                \"source_type\": \"path\"\n",
    "            }\n",
    "        \n",
    "        elif isinstance(item, bytes):\n",
    "            # Load from bytes\n",
    "            audio, sr = librosa.load(io.BytesIO(item), sr=self.sr)\n",
    "            audio_hash = hashlib.md5(item[:10000]).hexdigest()\n",
    "            pseudo_path = Path(f\"uploaded_{audio_hash}\")\n",
    "            \n",
    "            return {\n",
    "                \"audio\": audio,\n",
    "                \"sr\": sr,\n",
    "                \"path\": pseudo_path,\n",
    "                \"source_type\": \"bytes\"\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input type: {type(item)}\")\n",
    "    \n",
    "    def clear_cache(self, older_than_days: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Clear cache files\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        older_than_days : int, optional\n",
    "            If specified, only clear cache entries older than this many days.\n",
    "            If None, clears entire cache.\n",
    "        \"\"\"\n",
    "        if not self.use_cache:\n",
    "            return\n",
    "        \n",
    "        if older_than_days is None:\n",
    "            # Clear everything\n",
    "            for cache_file in self.cache_dir.glob(\"*.pkl\"):\n",
    "                try:\n",
    "                    cache_file.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self._cache_index = {}\n",
    "            self._memory_cache = {}\n",
    "            print(f\"Cleared all cache from {self.cache_dir}\")\n",
    "        else:\n",
    "            # Clear old entries\n",
    "            cutoff = np.datetime64('now') - np.timedelta64(older_than_days, 'D')\n",
    "            removed_count = 0\n",
    "            \n",
    "            for cache_key, info in list(self._cache_index.items()):\n",
    "                if info['last_access'] < cutoff:\n",
    "                    try:\n",
    "                        if info['file'].exists():\n",
    "                            info['file'].unlink()\n",
    "                        del self._cache_index[cache_key]\n",
    "                        if cache_key in self._memory_cache:\n",
    "                            del self._memory_cache[cache_key]\n",
    "                        removed_count += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            \n",
    "            print(f\"Removed {removed_count} cache entries older than {older_than_days} days\")\n",
    "        \n",
    "        self._save_cache_index()\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        if not self.use_cache:\n",
    "            return {\"cache_enabled\": False}\n",
    "        \n",
    "        total_size = sum(info['size_mb'] for info in self._cache_index.values())\n",
    "        \n",
    "        return {\n",
    "            \"cache_enabled\": True,\n",
    "            \"cache_dir\": str(self.cache_dir),\n",
    "            \"num_cached_items\": len(self._cache_index),\n",
    "            \"total_size_mb\": round(total_size, 2),\n",
    "            \"max_size_mb\": self.max_cache_size_mb,\n",
    "            \"utilization_pct\": round(total_size / self.max_cache_size_mb * 100, 1),\n",
    "            \"memory_cache_items\": len(self._memory_cache)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d61834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract MFCC features with statistics (mean, std, quartiles)\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 sr: int = 22050,\n",
    "                 n_mfcc: int = 13,\n",
    "                 n_fft: int = 2048,\n",
    "                 hop_length: int = 512,\n",
    "                 cache_dir: Path = None, \n",
    "                 enable_cache: bool = True\n",
    "                 ):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.cache_dir = cache_dir\n",
    "        self.enable_cache = enable_cache\n",
    "\n",
    "        if enable_cache and cache_dir:\n",
    "            cache_dir.mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract mfcc features from audio data\n",
    "        X: array of dicts from AudioLoader\n",
    "        \"\"\"\n",
    "        feature_vectors = []\n",
    "\n",
    "        for audio_dict in tqdm(X, desc='Extracting MFCC features'):\n",
    "            audio = audio_dict[\"audio\"]\n",
    "            audio_path = audio_dict[\"path\"]\n",
    "            source_type = audio_dict[\"source_type\"]\n",
    "\n",
    "            # Try cache\n",
    "            if self.enable_cache and source_type == \"path\":\n",
    "                cache_key = self._get_cache_key(audio_path)\n",
    "                cached = self._load_from_cache(cache_key)\n",
    "                if cached is not None:\n",
    "                    feature_vectors.append(cached)\n",
    "                    continue\n",
    "\n",
    "            # Extract features \n",
    "            features = self._extract_mfcc_stats(audio)\n",
    "\n",
    "            # Save to cache\n",
    "            if self.enable_cache and source_type==\"path\":\n",
    "                cache_key = self._get_cache_key(audio_path)\n",
    "                self._save_to_cache(cache_key, features)\n",
    "\n",
    "            feature_vectors.append(features)\n",
    "\n",
    "        return pd.DataFrame(feature_vectors)\n",
    "    \n",
    "\n",
    "    def _extract_mfcc_stats(self, audio: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Extract MFCC and compute statistics\"\"\"\n",
    "        # Compute MFCCs\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=audio,\n",
    "            sr=self.sr,\n",
    "            n_mfcc=self.n_mfcc,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # Compute deltas\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Compute statistics\n",
    "        stats = {}\n",
    "        \n",
    "        # MFCC statistics\n",
    "        for i in range(self.n_mfcc):\n",
    "            stats.update(self._make_stats(mfccs[i, :], f\"mfcc_{i}\"))\n",
    "        \n",
    "        # Delta statistics\n",
    "        for i in range(self.n_mfcc):\n",
    "            stats.update(self._make_stats(delta_mfccs[i, :], f\"delta_{i}\"))\n",
    "        \n",
    "        # Delta2 statistics\n",
    "        for i in range(self.n_mfcc):\n",
    "            stats.update(self._make_stats(delta2_mfccs[i, :], f\"delta2_{i}\"))\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_stats(feature_array: np.ndarray, name: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute statistics for a feature array\"\"\"\n",
    "        return {\n",
    "            f\"{name}_mean\": float(np.mean(feature_array)),\n",
    "            f\"{name}_std\": float(np.std(feature_array)),\n",
    "            f\"{name}_min\": float(np.min(feature_array)),\n",
    "            f\"{name}_max\": float(np.max(feature_array)),\n",
    "            f\"{name}_median\": float(np.median(feature_array)),\n",
    "            f\"{name}_q25\": float(np.percentile(feature_array, 25)),\n",
    "            f\"{name}_q75\": float(np.percentile(feature_array, 75))\n",
    "        }\n",
    "    \n",
    "    def _get_cache_key(self, audio_path: Path) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        params = f\"{audio_path}_{self.sr}_{self.n_mfcc}_{self.n_fft}_{self.hop_length}\"\n",
    "        return hashlib.md5(params.encode()).hexdigest()\n",
    "    \n",
    "    def _load_from_cache(self, cache_key: str) -> Dict[str, float]:\n",
    "        \"\"\"Load from cache\"\"\"\n",
    "        if not self.cache_dir:\n",
    "            return None\n",
    "        \n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, cache_key: str, features: Dict[str, float]):\n",
    "        \"\"\"Save to cache\"\"\"\n",
    "        if not self.cache_dir:\n",
    "            return\n",
    "        \n",
    "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(features, f)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03255cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerFeaturePredictor:\n",
    "    \"\"\"Train multiple models for each individual feature (not grouped)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        target_groups: Dict[str, Dict[str, List[str]]],\n",
    "        model_configs: Dict[str, Dict[str, Dict]],\n",
    "        n_iter_config: Dict[str, int],\n",
    "        use_scaler: bool = True\n",
    "    ):\n",
    "\n",
    "        self.target_groups = target_groups\n",
    "        self.model_configs = model_configs\n",
    "        self.n_iter_config = n_iter_config\n",
    "        self.use_scaler = use_scaler\n",
    "        self.scaler = StandardScaler() if use_scaler else None\n",
    "        \n",
    "        # NEW STRUCTURE: Store per feature instead of per group\n",
    "        # {task_type: {feature_name: {model_name: model}}}\n",
    "        self.trained_models = {'regression': {}, 'classification': {}}\n",
    "        \n",
    "        # {task_type: {feature_name: {'model_name': str, 'model': obj, 'score': float, 'group': str}}}\n",
    "        self.best_models = {'regression': {}, 'classification': {}}\n",
    "        \n",
    "        # {task_type: {feature_name: {model_name: {'best_score': ..., 'best_params': ..., 'cv_results': ...}}}}\n",
    "        self.training_results = {'regression': {}, 'classification': {}}\n",
    "        \n",
    "        # Keep track of which group each feature belongs to\n",
    "        self.feature_to_group = {'regression': {}, 'classification': {}}\n",
    "        self._build_feature_mapping()\n",
    "    \n",
    "    def _build_feature_mapping(self):\n",
    "        \"\"\"Create mapping of feature -> group for reference\"\"\"\n",
    "        for task_type in ['regression', 'classification']:\n",
    "            if task_type in self.target_groups:\n",
    "                for group_name, features in self.target_groups[task_type].items():\n",
    "                    for feature in features:\n",
    "                        self.feature_to_group[task_type][feature] = group_name\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.DataFrame, cv: int = 4, n_jobs: int = -1):\n",
    "        \"\"\"Train all models for all features\"\"\"\n",
    "        # Scale features\n",
    "        if self.use_scaler:\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.fit_transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        # Train regression models\n",
    "        if 'regression' in self.target_groups:\n",
    "            self._fit_task(\n",
    "                X_scaled, y, \n",
    "                task_type='regression',\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs\n",
    "            )\n",
    "        \n",
    "        # Train classification models\n",
    "        if 'classification' in self.target_groups:\n",
    "            self._fit_task(\n",
    "                X_scaled, y,\n",
    "                task_type='classification',\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs\n",
    "            )\n",
    "    \n",
    "    def _fit_task(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        task_type: str,\n",
    "        cv: int,\n",
    "        n_jobs: int\n",
    "    ):\n",
    "        \"\"\"Train all models for each individual feature in task type\"\"\"\n",
    "\n",
    "        print(f\"TRAINING {task_type.upper()} MODELS - PER FEATURE\")\n",
    "        \n",
    "        groups = self.target_groups[task_type]\n",
    "        models = self.model_configs[task_type]\n",
    "        \n",
    "        # Iterate through groups (for organization)\n",
    "        for group_name, target_cols in groups.items():\n",
    "\n",
    "            print(f\"GROUP: {group_name.upper()}\")\n",
    "            \n",
    "            # Train each feature individually\n",
    "            for feature_name in target_cols:\n",
    "                print(f\"\\n▶ FEATURE: {feature_name}\")\n",
    "                print(f\"  Training {len(models)} models...\\n\")\n",
    "                \n",
    "                y_feature = y[feature_name]\n",
    "                \n",
    "                # Initialize storage for this feature\n",
    "                self.trained_models[task_type][feature_name] = {}\n",
    "                self.training_results[task_type][feature_name] = {}\n",
    "                \n",
    "                best_score = -np.inf\n",
    "                best_model_name = None\n",
    "                \n",
    "                # Train each model type on this single feature\n",
    "                for model_name, config in models.items():\n",
    "                    try:\n",
    "                        # Always single output (one feature at a time)\n",
    "                        base = clone(config['base_model'])\n",
    "                        \n",
    "                        # Remove 'estimator__' prefix for single-output models\n",
    "                        param_grid = {\n",
    "                            key.replace('estimator__', ''): value \n",
    "                            for key, value in config['param_grid'].items()\n",
    "                        }\n",
    "                        \n",
    "                        # Setup BayesSearchCV\n",
    "                        n_iter = self.n_iter_config.get(model_name, 30)\n",
    "                        scoring = 'r2' if task_type == 'regression' else 'balanced_accuracy'\n",
    "                        \n",
    "                        search = BayesSearchCV(\n",
    "                            estimator=base,\n",
    "                            search_spaces=param_grid,\n",
    "                            n_iter=n_iter,\n",
    "                            cv=cv,\n",
    "                            scoring=scoring,\n",
    "                            n_jobs=n_jobs,\n",
    "                            random_state=42,\n",
    "                            verbose=0\n",
    "                        )\n",
    "                        \n",
    "                        # Fit on single feature\n",
    "                        search.fit(X, y_feature.values.ravel())\n",
    "                        \n",
    "                        # Store model\n",
    "                        self.trained_models[task_type][feature_name][model_name] = search.best_estimator_\n",
    "                        \n",
    "                        # Store results\n",
    "                        self.training_results[task_type][feature_name][model_name] = {\n",
    "                            'best_score': search.best_score_,\n",
    "                            'best_params': search.best_params_,\n",
    "                            'cv_results': search.cv_results_,\n",
    "                            'group': group_name\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"  ✓ {model_name:20s} {scoring}={search.best_score_:7.4f}\")\n",
    "                        \n",
    "                        # Track best model for this feature\n",
    "                        if search.best_score_ > best_score:\n",
    "                            best_score = search.best_score_\n",
    "                            best_model_name = model_name\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ✗ {model_name:20s} FAILED: {str(e)}\")\n",
    "                \n",
    "                # Set best model for this feature\n",
    "                if best_model_name:\n",
    "                    self.best_models[task_type][feature_name] = {\n",
    "                        'model_name': best_model_name,\n",
    "                        'model': self.trained_models[task_type][feature_name][best_model_name],\n",
    "                        'score': best_score,\n",
    "                        'group': group_name\n",
    "                    }\n",
    "                    print(f\"\\n  - BEST for '{feature_name}': {best_model_name} (score={best_score:.4f})\")\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        use_best: bool = True,\n",
    "        specific_models: Dict[str, Dict[str, str]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \n",
    "        if self.use_scaler:\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "        else:\n",
    "            X_scaled = X\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        for task_type in ['regression', 'classification']:\n",
    "            if task_type not in self.target_groups:\n",
    "                continue\n",
    "            \n",
    "            # Iterate through all features (not groups)\n",
    "            for group_name, features in self.target_groups[task_type].items():\n",
    "                for feature_name in features:\n",
    "                    # Select model for this feature\n",
    "                    if use_best:\n",
    "                        if feature_name not in self.best_models[task_type]:\n",
    "                            continue\n",
    "                        model = self.best_models[task_type][feature_name]['model']\n",
    "                    else:\n",
    "                        if specific_models and feature_name in specific_models.get(task_type, {}):\n",
    "                            model_name = specific_models[task_type][feature_name]\n",
    "                            model = self.trained_models[task_type][feature_name][model_name]\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    # Predict for this single feature\n",
    "                    predictions[feature_name] = model.predict(X_scaled)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        use_best: bool = True\n",
    "    ) -> Dict[str, Dict[str, Dict]]:\n",
    "        \"\"\"\n",
    "        Evaluate all models\n",
    "        \n",
    "        Returns:\n",
    "            {task_type: {feature_name: {metric: value}}}\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X, use_best=use_best)\n",
    "        results = {'regression': {}, 'classification': {}}\n",
    "        \n",
    "        # Evaluate regression\n",
    "        if 'regression' in self.target_groups:\n",
    "            for group_name, features in self.target_groups['regression'].items():\n",
    "                for feature_name in features:\n",
    "                    if feature_name in predictions:\n",
    "                        y_true = y[feature_name].values\n",
    "                        y_pred = predictions[feature_name]\n",
    "                        results['regression'][feature_name] = {\n",
    "                            'r2': r2_score(y_true, y_pred),\n",
    "                            'mae': mean_absolute_error(y_true, y_pred),\n",
    "                            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                            'group': self.feature_to_group['regression'][feature_name]\n",
    "                        }\n",
    "        \n",
    "        # Evaluate classification\n",
    "        if 'classification' in self.target_groups:\n",
    "            for group_name, features in self.target_groups['classification'].items():\n",
    "                for feature_name in features:\n",
    "                    if feature_name in predictions:\n",
    "                        y_true = y[feature_name].values\n",
    "                        y_pred = predictions[feature_name]\n",
    "                        results['classification'][feature_name] = {\n",
    "                            'accuracy': accuracy_score(y_true, y_pred),\n",
    "                            'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "                            'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "                            'group': self.feature_to_group['classification'][feature_name]\n",
    "                        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_results_dataframe(self, eval_results: Dict[str, Dict[str, Dict]]) -> pd.DataFrame:\n",
    "       \n",
    "        rows = []\n",
    "        \n",
    "        # Process regression results\n",
    "        if 'regression' in eval_results:\n",
    "            for feature_name, metrics in eval_results['regression'].items():\n",
    "                # Get the best model name for this feature\n",
    "                if feature_name in self.best_models['regression']:\n",
    "                    model_name = self.best_models['regression'][feature_name]['model_name']\n",
    "                    cv_score = self.best_models['regression'][feature_name]['score']\n",
    "                else:\n",
    "                    model_name = 'Unknown'\n",
    "                    cv_score = None\n",
    "                \n",
    "                rows.append({\n",
    "                    'task_type': 'regression',\n",
    "                    'feature': feature_name,\n",
    "                    'target_group': metrics.get('group', 'Unknown'),\n",
    "                    'model_name': model_name,\n",
    "                    'cv_score': cv_score,\n",
    "                    'r2_score': metrics['r2'],\n",
    "                    'mae': metrics['mae'],\n",
    "                    'rmse': metrics['rmse']\n",
    "                })\n",
    "        \n",
    "        # Process classification results\n",
    "        if 'classification' in eval_results:\n",
    "            for feature_name, metrics in eval_results['classification'].items():\n",
    "                if feature_name in self.best_models['classification']:\n",
    "                    model_name = self.best_models['classification'][feature_name]['model_name']\n",
    "                    cv_score = self.best_models['classification'][feature_name]['score']\n",
    "                else:\n",
    "                    model_name = 'Unknown'\n",
    "                    cv_score = None\n",
    "                \n",
    "                rows.append({\n",
    "                    'task_type': 'classification',\n",
    "                    'feature': feature_name,\n",
    "                    'target_group': metrics.get('group', 'Unknown'),\n",
    "                    'model_name': model_name,\n",
    "                    'cv_score': cv_score,\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'balanced_accuracy': metrics['balanced_accuracy'],\n",
    "                    'f1_weighted': metrics['f1_weighted']\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Sort by task type, group, and primary metric\n",
    "        if not df.empty:\n",
    "            sort_cols = ['task_type', 'target_group']\n",
    "            if 'r2_score' in df.columns:\n",
    "                sort_cols.append('r2_score')\n",
    "                df = df.sort_values(sort_cols, ascending=[True, True, False])\n",
    "            else:\n",
    "                df = df.sort_values(sort_cols, ascending=[True, True])\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_all_feature_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get training results for ALL models trained on each feature\n",
    "        (not just best model)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with all model results per feature\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for task_type in ['regression', 'classification']:\n",
    "            if task_type not in self.training_results:\n",
    "                continue\n",
    "            \n",
    "            for feature_name, models_dict in self.training_results[task_type].items():\n",
    "                for model_name, results in models_dict.items():\n",
    "                    row = {\n",
    "                        'task_type': task_type,\n",
    "                        'feature': feature_name,\n",
    "                        'group': results.get('group', 'Unknown'),\n",
    "                        'model_name': model_name,\n",
    "                        'cv_score': results['best_score'],\n",
    "                        'best_params': str(results['best_params']),\n",
    "                        'is_best': (\n",
    "                            feature_name in self.best_models[task_type] and \n",
    "                            self.best_models[task_type][feature_name]['model_name'] == model_name\n",
    "                        )\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        if not df.empty:\n",
    "            df = df.sort_values(['feature', 'cv_score'], ascending=[True, False])\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_models(self, save_dir: Path):\n",
    "        \"\"\"Save all models and metadata\"\"\"\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save scaler\n",
    "        if self.use_scaler and self.scaler is not None:\n",
    "            joblib.dump(self.scaler, save_dir / \"scaler.pkl\")\n",
    "            print(\"✓ Saved scaler\")\n",
    "        \n",
    "        # Save best models (per feature)\n",
    "        for task_type in ['regression', 'classification']:\n",
    "            for feature_name, info in self.best_models[task_type].items():\n",
    "                # Use feature name in filename\n",
    "                safe_name = feature_name.replace('/', '_').replace(' ', '_')\n",
    "                path = save_dir / f\"{task_type}_{safe_name}_best.pkl\"\n",
    "                joblib.dump(info, path)\n",
    "                print(f\"✓ Saved {task_type}/{feature_name}: {info['model_name']}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'target_groups': self.target_groups,\n",
    "            'feature_to_group': self.feature_to_group,\n",
    "            'best_models': {\n",
    "                task: {\n",
    "                    feature: {\n",
    "                        'model_name': info['model_name'], \n",
    "                        'score': info['score'],\n",
    "                        'group': info['group']\n",
    "                    }\n",
    "                    for feature, info in features.items()\n",
    "                }\n",
    "                for task, features in self.best_models.items()\n",
    "            }\n",
    "        }\n",
    "        joblib.dump(metadata, save_dir / \"metadata.pkl\")\n",
    "        print(\"✓ Saved metadata\")\n",
    "    \n",
    "    def load_models(self, load_dir: Path):\n",
    "        \"\"\"Load saved models\"\"\"\n",
    "        load_dir = Path(load_dir)\n",
    "        \n",
    "        # Load scaler\n",
    "        if self.use_scaler:\n",
    "            scaler_path = load_dir / \"scaler.pkl\"\n",
    "            if scaler_path.exists():\n",
    "                self.scaler = joblib.load(scaler_path)\n",
    "                print(\"✓ Loaded scaler\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata = joblib.load(load_dir / \"metadata.pkl\")\n",
    "        self.feature_to_group = metadata.get('feature_to_group', self.feature_to_group)\n",
    "        \n",
    "        # Load best models (per feature)\n",
    "        for task_type in ['regression', 'classification']:\n",
    "            if task_type not in metadata['best_models']:\n",
    "                continue\n",
    "            \n",
    "            for feature_name in metadata['best_models'][task_type].keys():\n",
    "                safe_name = feature_name.replace('/', '_').replace(' ', '_')\n",
    "                path = load_dir / f\"{task_type}_{safe_name}_best.pkl\"\n",
    "                if path.exists():\n",
    "                    self.best_models[task_type][feature_name] = joblib.load(path)\n",
    "                    print(f\"✓ Loaded {task_type}/{feature_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a433f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicFeaturePipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Audio → MFCC → (Optional PCA) → Multi-Model Predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_loader,  # AudioLoader instance\n",
    "        mfcc_extractor,  # MFCCExtractor instance\n",
    "        target_groups: Dict[str, Dict[str, List[str]]],\n",
    "        model_configs: Dict[str, Dict[str, Dict]],\n",
    "        n_iter_config: Dict[str, int],\n",
    "        model_dir: Path = None,\n",
    "        use_scaler: bool = True,\n",
    "        use_pca: bool = False,\n",
    "        n_components: Optional[Union[int, float]] = None\n",
    "    ):\n",
    "        \n",
    "        self.audio_loader = audio_loader\n",
    "        self.mfcc_extractor = mfcc_extractor\n",
    "        self.use_pca = use_pca\n",
    "        self.n_components = n_components\n",
    "        self.pca = None\n",
    "        \n",
    "        # Unified Predictor\n",
    "        self.predictor = PerFeaturePredictor(\n",
    "            target_groups=target_groups,\n",
    "            model_configs=model_configs,\n",
    "            n_iter_config=n_iter_config,\n",
    "            use_scaler=use_scaler\n",
    "        )\n",
    "        self.model_dir = Path(model_dir) if model_dir else None\n",
    "    \n",
    "    def _apply_pca(self, features: pd.DataFrame, fit: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply PCA transformation to features\n",
    "        \"\"\"\n",
    "        if fit:\n",
    "            self.pca = PCA(n_components=self.n_components, random_state=42)\n",
    "            features_transformed = self.pca.fit_transform(features)\n",
    "            print(f\"  PCA fitted: {features.shape[1]} → {self.pca.n_components_} components\")\n",
    "            print(f\"  Explained variance: {self.pca.explained_variance_ratio_.sum():.4f}\")\n",
    "        else:\n",
    "            if self.pca is None:\n",
    "                raise ValueError(\"PCA not fitted. Call with fit=True first.\")\n",
    "            features_transformed = self.pca.transform(features)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        features_df = pd.DataFrame(\n",
    "            features_transformed,\n",
    "            columns=[f'pca_{i}' for i in range(features_transformed.shape[1])],\n",
    "            index=features.index\n",
    "        )\n",
    "        return features_df\n",
    "    \n",
    "    def extract_features(\n",
    "        self, \n",
    "        audio_paths: List[Union[str, Path]], \n",
    "        apply_pca: bool = None,\n",
    "        fit_pca: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "        if apply_pca is None:\n",
    "            apply_pca = self.use_pca\n",
    "        \n",
    "        print(\"Loading audio files...\")\n",
    "        audio_data = self.audio_loader.transform(audio_paths)\n",
    "        \n",
    "        print(\"Extracting MFCC features...\")\n",
    "        features = self.mfcc_extractor.transform(audio_data)\n",
    "        \n",
    "        if apply_pca:\n",
    "            print(\"Applying PCA...\")\n",
    "            features = self._apply_pca(features, fit=fit_pca)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        audio_paths: List[Union[str, Path]],\n",
    "        targets: pd.DataFrame,\n",
    "        cv: int = 4,\n",
    "        n_jobs: int = -1\n",
    "    ):\n",
    "       \n",
    "        # Extract features (with PCA if enabled)\n",
    "        X = self.extract_features(audio_paths, fit_pca=self.use_pca)\n",
    "        \n",
    "        # Ensure alignment\n",
    "        if len(X) != len(targets):\n",
    "            raise ValueError(f\"Mismatch: {len(X)} audio samples but {len(targets)} target rows\")\n",
    "        \n",
    "        self.predictor.fit(X, targets, cv=cv, n_jobs=n_jobs)\n",
    "        \n",
    "        # Save models\n",
    "        if self.model_dir:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SAVING MODELS\")\n",
    "            print(\"=\"*60)\n",
    "            self._save_pipeline()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        audio_paths: List[Union[str, Path]],\n",
    "        use_best: bool = True\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \n",
    "        # Load models if needed\n",
    "        if not self.predictor.best_models['regression'] and not self.predictor.best_models['classification']:\n",
    "            if self.model_dir and self.model_dir.exists():\n",
    "                print(\"Loading models...\")\n",
    "                self._load_pipeline()\n",
    "            else:\n",
    "                raise ValueError(\"No models trained or loaded. Call train() first or provide valid model_dir\")\n",
    "        \n",
    "        # Extract features (with PCA if enabled, but don't fit)\n",
    "        X = self.extract_features(audio_paths, apply_pca=self.use_pca, fit_pca=False)\n",
    "        \n",
    "        # Predict\n",
    "        print(\"Generating predictions...\")\n",
    "        predictions = self.predictor.predict(X, use_best=use_best)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        audio_paths: List[Union[str, Path]],\n",
    "        targets: pd.DataFrame,\n",
    "        use_best: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "       \n",
    "        # Extract features (with PCA if enabled, but don't fit)\n",
    "        X = self.extract_features(audio_paths, apply_pca=self.use_pca, fit_pca=False)\n",
    "        \n",
    "        # Ensure alignment\n",
    "        if len(X) != len(targets):\n",
    "            raise ValueError(f\"Mismatch: {len(X)} audio samples but {len(targets)} target rows\")\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = self.predictor.evaluate(X, targets, use_best=use_best)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        results_df = self.predictor.get_results_dataframe(eval_results)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def get_training_summary(self) -> pd.DataFrame:\n",
    "       \n",
    "        return self.predictor.get_all_feature_results()\n",
    "    \n",
    "    def _save_pipeline(self):\n",
    "        \"\"\"Save complete pipeline including PCA\"\"\"\n",
    "        if not self.model_dir:\n",
    "            raise ValueError(\"model_dir not specified\")\n",
    "        \n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save predictor models\n",
    "        self.predictor.save_models(self.model_dir)\n",
    "        \n",
    "        # Save PCA if used\n",
    "        if self.use_pca and self.pca is not None:\n",
    "            joblib.dump(self.pca, self.model_dir / \"pca.pkl\")\n",
    "            print(\"✓ Saved PCA transformer\")\n",
    "        \n",
    "        # Save pipeline config\n",
    "        pipeline_config = {\n",
    "            'use_pca': self.use_pca,\n",
    "            'n_components': self.n_components\n",
    "        }\n",
    "        joblib.dump(pipeline_config, self.model_dir / \"pipeline_config.pkl\")\n",
    "        print(\"✓ Saved pipeline configuration\")\n",
    "    \n",
    "    def _load_pipeline(self):\n",
    "        \"\"\"Load complete pipeline including PCA\"\"\"\n",
    "        if not self.model_dir:\n",
    "            raise ValueError(\"model_dir not specified\")\n",
    "        \n",
    "        # Load predictor models\n",
    "        self.predictor.load_models(self.model_dir)\n",
    "        \n",
    "        # Load pipeline config\n",
    "        config_path = self.model_dir / \"pipeline_config.pkl\"\n",
    "        if config_path.exists():\n",
    "            pipeline_config = joblib.load(config_path)\n",
    "            self.use_pca = pipeline_config.get('use_pca', False)\n",
    "            self.n_components = pipeline_config.get('n_components', None)\n",
    "            print(\"✓ Loaded pipeline configuration\")\n",
    "        \n",
    "        # Load PCA if used\n",
    "        if self.use_pca:\n",
    "            pca_path = self.model_dir / \"pca.pkl\"\n",
    "            if pca_path.exists():\n",
    "                self.pca = joblib.load(pca_path)\n",
    "                print(\"✓ Loaded PCA transformer\")\n",
    "            else:\n",
    "                raise ValueError(\"PCA enabled but pca.pkl not found in model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4183c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_groups = {\n",
    "    \"regression\": {\n",
    "        \"energy_mood\": [\"energy\", \"valence\", \"danceability\"],\n",
    "        \"production\": [\"loudness\", \"acousticness\", \"instrumentalness\", \"liveness\"],\n",
    "        \"structure\": [\"speechiness\"],\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"key\": [\"key\"],\n",
    "        \"mode\": [\"mode\"], \n",
    "        \"tempo_bins\": [\"tempo_bins\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"regression\": {\n",
    "        \"Ridge\": {\n",
    "            \"base_model\": Ridge(),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__alpha\": Real(0.1, 10.0, prior=\"log-uniform\"),\n",
    "                \"estimator__solver\": Categorical([\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]),\n",
    "            },\n",
    "        },\n",
    "        \"Lasso\": {\n",
    "            \"base_model\": Lasso(),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__alpha\": Real(0.0001, 1.0, prior=\"log-uniform\"),\n",
    "                \"estimator__selection\": Categorical([\"cyclic\", \"random\"]),\n",
    "            },\n",
    "        },\n",
    "        \"ElasticNet\": {\n",
    "            \"base_model\": ElasticNet(),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__alpha\": Real(0.0001, 1.0, prior=\"log-uniform\"),\n",
    "                \"estimator__l1_ratio\": Real(0.1, 1.0),\n",
    "                \"estimator__selection\": Categorical([\"cyclic\", \"random\"]),\n",
    "            },\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"base_model\": RandomForestRegressor(random_state=42),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__n_estimators\": Integer(150, 350),\n",
    "                \"estimator__max_depth\": Integer(10, 20),\n",
    "                \"estimator__min_samples_split\": Integer(2, 8),\n",
    "                \"estimator__min_samples_leaf\": Integer(3, 8),\n",
    "                \"estimator__max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
    "            },\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            \"base_model\": SVR(kernel=\"rbf\"),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__C\": Real(0.1, 100.0, prior=\"log-uniform\"),\n",
    "                \"estimator__gamma\": Real(0.00001, 0.01, prior=\"log-uniform\"),\n",
    "                \"estimator__epsilon\": Real(0.01, 0.2),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"Random Forest\": {\n",
    "            \"base_model\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__n_estimators\": Integer(150, 350),\n",
    "                \"estimator__max_depth\": Integer(8, 20),\n",
    "                \"estimator__min_samples_split\": Integer(2, 10),\n",
    "                \"estimator__min_samples_leaf\": Integer(3, 6),\n",
    "                \"estimator__max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
    "                \"estimator__bootstrap\": Categorical([True, False]),\n",
    "            },\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            \"base_model\": SVC(kernel=\"rbf\", probability=True, random_state=42, class_weight='balanced'),\n",
    "            \"param_grid\": {\n",
    "                \"estimator__C\": Real(1.0, 20.0, prior=\"log-uniform\"),\n",
    "                \"estimator__gamma\": Real(0.001, 0.02, prior=\"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "n_iter_config = {\n",
    "    \"Ridge\": 30,\n",
    "    \"Lasso\": 30,\n",
    "    \"ElasticNet\": 30,\n",
    "    \"Random Forest\": 50,  \n",
    "    \"SVM\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3ff70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick path fix\n",
    "def fix_path(path):\n",
    "    \"\"\"Fix WSL path for Windows access\"\"\"\n",
    "    path_str = str(path)\n",
    "    if '\\\\\\\\wsl.localhost\\\\' in path_str:\n",
    "        # Try alternative format\n",
    "        path_str = path_str.replace('\\\\\\\\wsl.localhost\\\\', '\\\\\\\\wsl$\\\\')\n",
    "    return path_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c7e9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.916</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.162</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.640</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.799</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.192</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.445</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.584</td>\n",
       "      <td>0.918</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.646</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.022</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.606   0.916    6    -8.162     1       0.0371        0.1400   \n",
       "1         0.280   0.640   11    -7.799     0       0.1230        0.3490   \n",
       "2         0.192   0.411    2    -9.445     1       0.0655        0.5390   \n",
       "3         0.584   0.918    7    -9.883     1       0.0345        0.0254   \n",
       "4         0.415   0.646    2   -12.022     1       0.0399        0.0189   \n",
       "\n",
       "   instrumentalness  liveness  valence tempo_bins  \n",
       "0             0.356    0.1320   0.8890          2  \n",
       "1             0.675    0.1360   0.0537          4  \n",
       "2             0.709    0.0909   0.1390          0  \n",
       "3             0.770    0.3480   0.1140          2  \n",
       "4             0.948    0.0965   0.1230          1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data = pd.read_csv(prc / \"matched_metadata.csv\")\n",
    "audio_data.head()\n",
    "\n",
    "bins = [0, 80, 100, 120, 140, 170, float(\"inf\")]\n",
    "numeric_labels = [0, 1, 2, 3, 4, 5]  # Now 6 labels for 6 bins\n",
    "\n",
    "audio_data[\"tempo_bins\"] = pd.cut(\n",
    "    audio_data[\"tempo\"], bins=bins, labels=numeric_labels, right=False\n",
    ")\n",
    "\n",
    "\n",
    "X = audio_data[\"track_id\"].map(lambda id: asp / f\"{str(id).zfill(6)}.mp3\")\n",
    "y = audio_data[\n",
    "    [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo_bins\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078b598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1383\n",
      "Test samples: 346\n",
      "Loading audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|██████████| 1383/1383 [1:25:24<00:00,  3.71s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 56 hits, 1327 misses (4.0% hit rate)\n",
      "Extracting MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features: 100%|██████████| 1383/1383 [00:50<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "  PCA fitted: 273 → 80 components\n",
      "  Explained variance: 0.9998\n",
      "TRAINING REGRESSION MODELS - PER FEATURE\n",
      "GROUP: ENERGY_MOOD\n",
      "\n",
      "▶ FEATURE: energy\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.5032\n",
      "  ✓ Lasso                r2= 0.5071\n",
      "  ✓ ElasticNet           r2= 0.5073\n",
      "  ✓ Random Forest        r2= 0.4896\n",
      "  ✓ SVM                  r2= 0.5372\n",
      "\n",
      "  - BEST for 'energy': SVM (score=0.5372)\n",
      "\n",
      "▶ FEATURE: valence\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.2417\n",
      "  ✓ Lasso                r2= 0.2577\n",
      "  ✓ ElasticNet           r2= 0.2575\n",
      "  ✓ Random Forest        r2= 0.2078\n",
      "  ✓ SVM                  r2= 0.2852\n",
      "\n",
      "  - BEST for 'valence': SVM (score=0.2852)\n",
      "\n",
      "▶ FEATURE: danceability\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.3989\n",
      "  ✓ Lasso                r2= 0.4085\n",
      "  ✓ ElasticNet           r2= 0.4086\n",
      "  ✓ Random Forest        r2= 0.3497\n",
      "  ✓ SVM                  r2= 0.4284\n",
      "\n",
      "  - BEST for 'danceability': SVM (score=0.4284)\n",
      "GROUP: PRODUCTION\n",
      "\n",
      "▶ FEATURE: loudness\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.3630\n",
      "  ✓ Lasso                r2= 0.3788\n",
      "  ✓ ElasticNet           r2= 0.3785\n",
      "  ✓ Random Forest        r2= 0.4247\n",
      "  ✓ SVM                  r2= 0.4086\n",
      "\n",
      "  - BEST for 'loudness': Random Forest (score=0.4247)\n",
      "\n",
      "▶ FEATURE: acousticness\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.4697\n",
      "  ✓ Lasso                r2= 0.4782\n",
      "  ✓ ElasticNet           r2= 0.4778\n",
      "  ✓ Random Forest        r2= 0.4214\n",
      "  ✓ SVM                  r2= 0.4746\n",
      "\n",
      "  - BEST for 'acousticness': Lasso (score=0.4782)\n",
      "\n",
      "▶ FEATURE: instrumentalness\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.1716\n",
      "  ✓ Lasso                r2= 0.1899\n",
      "  ✓ ElasticNet           r2= 0.1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(20), None, np.int64(3), np.int64(8), np.int64(150)] before, using random point [np.int64(14), 'sqrt', np.int64(8), np.int64(6), np.int64(257)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Random Forest        r2= 0.1981\n",
      "  ✓ SVM                  r2= 0.2105\n",
      "\n",
      "  - BEST for 'instrumentalness': SVM (score=0.2105)\n",
      "\n",
      "▶ FEATURE: liveness\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2=-0.0408\n",
      "  ✓ Lasso                r2= 0.0227\n",
      "  ✓ ElasticNet           r2= 0.0217\n",
      "  ✓ Random Forest        r2= 0.0183\n",
      "  ✓ SVM                  r2= 0.0088\n",
      "\n",
      "  - BEST for 'liveness': Lasso (score=0.0227)\n",
      "GROUP: STRUCTURE\n",
      "\n",
      "▶ FEATURE: speechiness\n",
      "  Training 5 models...\n",
      "\n",
      "  ✓ Ridge                r2= 0.0241\n",
      "  ✓ Lasso                r2= 0.0765\n",
      "  ✓ ElasticNet           r2= 0.0765\n",
      "  ✓ Random Forest        r2= 0.0630\n",
      "  ✓ SVM                  r2= 0.0896\n",
      "\n",
      "  - BEST for 'speechiness': SVM (score=0.0896)\n",
      "TRAINING CLASSIFICATION MODELS - PER FEATURE\n",
      "GROUP: KEY\n",
      "\n",
      "▶ FEATURE: key\n",
      "  Training 2 models...\n",
      "\n",
      "  ✓ Random Forest        balanced_accuracy= 0.1312\n",
      "  ✓ SVM                  balanced_accuracy= 0.1375\n",
      "\n",
      "  - BEST for 'key': SVM (score=0.1375)\n",
      "GROUP: MODE\n",
      "\n",
      "▶ FEATURE: mode\n",
      "  Training 2 models...\n",
      "\n",
      "  ✓ Random Forest        balanced_accuracy= 0.5480\n",
      "  ✓ SVM                  balanced_accuracy= 0.5457\n",
      "\n",
      "  - BEST for 'mode': Random Forest (score=0.5480)\n",
      "GROUP: TEMPO_BINS\n",
      "\n",
      "▶ FEATURE: tempo_bins\n",
      "  Training 2 models...\n",
      "\n",
      "  ✓ Random Forest        balanced_accuracy= 0.2624\n",
      "  ✓ SVM                  balanced_accuracy= 0.2676\n",
      "\n",
      "  - BEST for 'tempo_bins': SVM (score=0.2676)\n",
      "\n",
      "============================================================\n",
      "SAVING MODELS\n",
      "============================================================\n",
      "✓ Saved scaler\n",
      "✓ Saved regression/energy: SVM\n",
      "✓ Saved regression/valence: SVM\n",
      "✓ Saved regression/danceability: SVM\n",
      "✓ Saved regression/loudness: Random Forest\n",
      "✓ Saved regression/acousticness: Lasso\n",
      "✓ Saved regression/instrumentalness: SVM\n",
      "✓ Saved regression/liveness: Lasso\n",
      "✓ Saved regression/speechiness: SVM\n",
      "✓ Saved classification/key: SVM\n",
      "✓ Saved classification/mode: Random Forest\n",
      "✓ Saved classification/tempo_bins: SVM\n",
      "✓ Saved metadata\n",
      "✓ Saved PCA transformer\n",
      "✓ Saved pipeline configuration\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Loading audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|██████████| 346/346 [04:37<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 14 hits, 332 misses (4.0% hit rate)\n",
      "Extracting MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features: 100%|██████████| 346/346 [00:11<00:00, 28.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "TEST SET RESULTS\n",
      "         task_type           feature target_group     model_name  cv_score  \\\n",
      "0   classification               key          key            SVM  0.137535   \n",
      "1   classification              mode         mode  Random Forest  0.547999   \n",
      "2   classification        tempo_bins   tempo_bins            SVM  0.267584   \n",
      "3       regression            energy  energy_mood            SVM  0.537209   \n",
      "4       regression      danceability  energy_mood            SVM  0.428387   \n",
      "5       regression           valence  energy_mood            SVM  0.285222   \n",
      "6       regression      acousticness   production          Lasso  0.478154   \n",
      "7       regression          loudness   production  Random Forest  0.424675   \n",
      "8       regression  instrumentalness   production            SVM  0.210457   \n",
      "9       regression          liveness   production          Lasso  0.022731   \n",
      "10      regression       speechiness    structure            SVM  0.089577   \n",
      "\n",
      "    r2_score       mae      rmse  accuracy  balanced_accuracy  f1_weighted  \n",
      "0        NaN       NaN       NaN  0.164740           0.130025     0.153378  \n",
      "1        NaN       NaN       NaN  0.670520           0.525984     0.596745  \n",
      "2        NaN       NaN       NaN  0.268786           0.260895     0.265692  \n",
      "3   0.567478  0.139898  0.177682       NaN                NaN          NaN  \n",
      "4   0.404007  0.108666  0.132719       NaN                NaN          NaN  \n",
      "5   0.299726  0.170067  0.214129       NaN                NaN          NaN  \n",
      "6   0.430318  0.214942  0.272346       NaN                NaN          NaN  \n",
      "7   0.413790  2.707923  3.508040       NaN                NaN          NaN  \n",
      "8   0.246843  0.263878  0.318752       NaN                NaN          NaN  \n",
      "9   0.038284  0.112209  0.153257       NaN                NaN          NaN  \n",
      "10  0.092203  0.029863  0.050710       NaN                NaN          NaN  \n",
      "ALL TRAINING RESULTS (CV SCORES)\n",
      "         task_type           feature        group     model_name  cv_score  \\\n",
      "0       regression      acousticness   production          Lasso  0.478154   \n",
      "1       regression      acousticness   production     ElasticNet  0.477790   \n",
      "2       regression      acousticness   production            SVM  0.474586   \n",
      "3       regression      acousticness   production          Ridge  0.469744   \n",
      "4       regression      acousticness   production  Random Forest  0.421410   \n",
      "5       regression      danceability  energy_mood            SVM  0.428387   \n",
      "6       regression      danceability  energy_mood     ElasticNet  0.408600   \n",
      "7       regression      danceability  energy_mood          Lasso  0.408463   \n",
      "8       regression      danceability  energy_mood          Ridge  0.398913   \n",
      "9       regression      danceability  energy_mood  Random Forest  0.349739   \n",
      "10      regression            energy  energy_mood            SVM  0.537209   \n",
      "11      regression            energy  energy_mood     ElasticNet  0.507288   \n",
      "12      regression            energy  energy_mood          Lasso  0.507126   \n",
      "13      regression            energy  energy_mood          Ridge  0.503228   \n",
      "14      regression            energy  energy_mood  Random Forest  0.489576   \n",
      "15      regression  instrumentalness   production            SVM  0.210457   \n",
      "16      regression  instrumentalness   production  Random Forest  0.198092   \n",
      "17      regression  instrumentalness   production     ElasticNet  0.190113   \n",
      "18      regression  instrumentalness   production          Lasso  0.189914   \n",
      "19      regression  instrumentalness   production          Ridge  0.171560   \n",
      "20  classification               key          key            SVM  0.137535   \n",
      "21  classification               key          key  Random Forest  0.131216   \n",
      "22      regression          liveness   production          Lasso  0.022731   \n",
      "23      regression          liveness   production     ElasticNet  0.021689   \n",
      "24      regression          liveness   production  Random Forest  0.018275   \n",
      "25      regression          liveness   production            SVM  0.008780   \n",
      "26      regression          liveness   production          Ridge -0.040834   \n",
      "27      regression          loudness   production  Random Forest  0.424675   \n",
      "28      regression          loudness   production            SVM  0.408560   \n",
      "29      regression          loudness   production          Lasso  0.378764   \n",
      "30      regression          loudness   production     ElasticNet  0.378509   \n",
      "31      regression          loudness   production          Ridge  0.363003   \n",
      "32  classification              mode         mode  Random Forest  0.547999   \n",
      "33  classification              mode         mode            SVM  0.545707   \n",
      "34      regression       speechiness    structure            SVM  0.089577   \n",
      "35      regression       speechiness    structure     ElasticNet  0.076497   \n",
      "36      regression       speechiness    structure          Lasso  0.076479   \n",
      "37      regression       speechiness    structure  Random Forest  0.062951   \n",
      "38      regression       speechiness    structure          Ridge  0.024115   \n",
      "39  classification        tempo_bins   tempo_bins            SVM  0.267584   \n",
      "40  classification        tempo_bins   tempo_bins  Random Forest  0.262426   \n",
      "41      regression           valence  energy_mood            SVM  0.285222   \n",
      "42      regression           valence  energy_mood          Lasso  0.257679   \n",
      "43      regression           valence  energy_mood     ElasticNet  0.257548   \n",
      "44      regression           valence  energy_mood          Ridge  0.241690   \n",
      "45      regression           valence  energy_mood  Random Forest  0.207844   \n",
      "\n",
      "                                                                                                                                       best_params  \\\n",
      "0                                                                              OrderedDict({'alpha': 0.004369339947510315, 'selection': 'random'})   \n",
      "1                                              OrderedDict({'alpha': 0.0077940057488407365, 'l1_ratio': 0.655730797939661, 'selection': 'cyclic'})   \n",
      "2                                                             OrderedDict({'C': 46.33325216272167, 'epsilon': 0.2, 'gamma': 0.000455282824557108})   \n",
      "3                                                                                                    OrderedDict({'alpha': 10.0, 'solver': 'sag'})   \n",
      "4                         OrderedDict({'max_depth': 20, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 150})   \n",
      "5                                                           OrderedDict({'C': 0.4730153291208055, 'epsilon': 0.01, 'gamma': 0.003457607345564371})   \n",
      "6                                                             OrderedDict({'alpha': 0.016550179204029736, 'l1_ratio': 0.1, 'selection': 'cyclic'})   \n",
      "7                                                                             OrderedDict({'alpha': 0.0018775654486971756, 'selection': 'cyclic'})   \n",
      "8                                                                                                   OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "9                         OrderedDict({'max_depth': 17, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 3, 'n_estimators': 310})   \n",
      "10                                           OrderedDict({'C': 0.8799001209035012, 'epsilon': 0.07151190856888412, 'gamma': 0.004267836399541129})   \n",
      "11                                                              OrderedDict({'alpha': 0.0216597782294129, 'l1_ratio': 0.1, 'selection': 'random'})   \n",
      "12                                                                            OrderedDict({'alpha': 0.0019431220277867235, 'selection': 'random'})   \n",
      "13                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "14                        OrderedDict({'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 212})   \n",
      "15                                           OrderedDict({'C': 0.37069661631625506, 'epsilon': 0.11388610246501389, 'gamma': 0.00731449342355713})   \n",
      "16                        OrderedDict({'max_depth': 20, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 150})   \n",
      "17                                              OrderedDict({'alpha': 0.01662162158460663, 'l1_ratio': 0.3046152167305568, 'selection': 'cyclic'})   \n",
      "18                                                                             OrderedDict({'alpha': 0.005884665482732549, 'selection': 'random'})   \n",
      "19                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "20                                                                           OrderedDict({'C': 19.986711327895424, 'gamma': 0.010446417604534108})   \n",
      "21   OrderedDict({'bootstrap': True, 'max_depth': 8, 'max_features': 'log2', 'min_samples_leaf': 6, 'min_samples_split': 10, 'n_estimators': 337})   \n",
      "22                                                                             OrderedDict({'alpha': 0.009407972242855412, 'selection': 'cyclic'})   \n",
      "23                                             OrderedDict({'alpha': 0.010167133973853923, 'l1_ratio': 0.7858741685947536, 'selection': 'cyclic'})   \n",
      "24                      OrderedDict({'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 150})   \n",
      "25                                                          OrderedDict({'C': 14.643343495369106, 'epsilon': 0.10111967792912141, 'gamma': 1e-05})   \n",
      "26                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "27                        OrderedDict({'max_depth': 14, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 188})   \n",
      "28                                                           OrderedDict({'C': 16.671280037501674, 'epsilon': 0.2, 'gamma': 0.004519512881249562})   \n",
      "29                                                                              OrderedDict({'alpha': 0.07449566343368977, 'selection': 'cyclic'})   \n",
      "30                                              OrderedDict({'alpha': 0.08632012725909878, 'l1_ratio': 0.9454327638424945, 'selection': 'cyclic'})   \n",
      "31                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "32  OrderedDict({'bootstrap': False, 'max_depth': 19, 'max_features': 'log2', 'min_samples_leaf': 6, 'min_samples_split': 5, 'n_estimators': 160})   \n",
      "33                                                                                           OrderedDict({'C': 1.504264775088212, 'gamma': 0.001})   \n",
      "34                                           OrderedDict({'C': 60.5214062972065, 'epsilon': 0.02139333307605589, 'gamma': 0.00015755396722878573})   \n",
      "35                                            OrderedDict({'alpha': 0.0042231518105311444, 'l1_ratio': 0.4822804748345446, 'selection': 'random'})   \n",
      "36                                                                            OrderedDict({'alpha': 0.0021050048491194923, 'selection': 'cyclic'})   \n",
      "37                      OrderedDict({'max_depth': 18, 'max_features': 'sqrt', 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 150})   \n",
      "38                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "39                                                                          OrderedDict({'C': 3.0249194238453407, 'gamma': 0.0068420002336432376})   \n",
      "40     OrderedDict({'bootstrap': True, 'max_depth': 11, 'max_features': None, 'min_samples_leaf': 6, 'min_samples_split': 8, 'n_estimators': 350})   \n",
      "41                                          OrderedDict({'C': 0.6525803321054942, 'epsilon': 0.09092684812930901, 'gamma': 0.0042787887247930905})   \n",
      "42                                                                             OrderedDict({'alpha': 0.004008209396708254, 'selection': 'random'})   \n",
      "43                                             OrderedDict({'alpha': 0.008422086759027394, 'l1_ratio': 0.5069869226818874, 'selection': 'cyclic'})   \n",
      "44                                                                                                  OrderedDict({'alpha': 10.0, 'solver': 'saga'})   \n",
      "45                        OrderedDict({'max_depth': 20, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 8, 'n_estimators': 350})   \n",
      "\n",
      "    is_best  \n",
      "0      True  \n",
      "1     False  \n",
      "2     False  \n",
      "3     False  \n",
      "4     False  \n",
      "5      True  \n",
      "6     False  \n",
      "7     False  \n",
      "8     False  \n",
      "9     False  \n",
      "10     True  \n",
      "11    False  \n",
      "12    False  \n",
      "13    False  \n",
      "14    False  \n",
      "15     True  \n",
      "16    False  \n",
      "17    False  \n",
      "18    False  \n",
      "19    False  \n",
      "20     True  \n",
      "21    False  \n",
      "22     True  \n",
      "23    False  \n",
      "24    False  \n",
      "25    False  \n",
      "26    False  \n",
      "27     True  \n",
      "28    False  \n",
      "29    False  \n",
      "30    False  \n",
      "31    False  \n",
      "32     True  \n",
      "33    False  \n",
      "34     True  \n",
      "35    False  \n",
      "36    False  \n",
      "37    False  \n",
      "38    False  \n",
      "39     True  \n",
      "40    False  \n",
      "41     True  \n",
      "42    False  \n",
      "43    False  \n",
      "44    False  \n",
      "45    False  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    audio_loader = AudioLoader(sr=22050, cache_dir=intr / \"MFCC_cache\")\n",
    "    mfcc_extractor = MFCCExtractor(n_mfcc=13)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    X_train_fixed = [fix_path(p) for p in X_train]\n",
    "    X_test_fixed = [fix_path(p) for p in X_test]\n",
    "    \n",
    "    # Create complete pipeline with PCA\n",
    "    pipeline = MusicFeaturePipeline(\n",
    "        audio_loader=audio_loader,\n",
    "        mfcc_extractor=mfcc_extractor,\n",
    "        target_groups=target_groups,\n",
    "        model_configs=model_configs,\n",
    "        n_iter_config=n_iter_config,\n",
    "        model_dir=Path(\"models/per_feature/\"),\n",
    "        use_scaler=True,\n",
    "        use_pca=True,           # Enable PCA\n",
    "        n_components=80         # 80 components (or use 0.95 for 95% variance)\n",
    "    )\n",
    "    \n",
    "    # Train: Audio → MFCC → PCA → Models\n",
    "    pipeline.train(X_train_fixed, y_train, cv=4, n_jobs=-1)\n",
    "    \n",
    "    # Evaluate: Audio → MFCC → PCA → Predictions → Metrics\n",
    "    results_df = pipeline.evaluate(X_test_fixed, y_test)\n",
    "    print(\"TEST SET RESULTS\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Get all training results\n",
    "    all_results = pipeline.get_training_summary()\n",
    "    print(\"ALL TRAINING RESULTS (CV SCORES)\")\n",
    "\n",
    "    print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eb015ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(predictions: Dict[str, np.ndarray], decimals: int = 2) -> Dict[str, Union[float, int]]:\n",
    "    formatted = {}\n",
    "    \n",
    "    for feature, value in predictions.items():\n",
    "        # Extract scalar from array\n",
    "        scalar_value = value.item() if hasattr(value, 'item') else value[0]\n",
    "        \n",
    "        # Format based on type\n",
    "        if isinstance(scalar_value, (np.integer, int)):\n",
    "            formatted[feature] = int(scalar_value)\n",
    "        else:\n",
    "            formatted[feature] = round(float(scalar_value), decimals)\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def print_predictions(predictions: Dict[str, np.ndarray], decimals: int = 2):\n",
    "    formatted = format_predictions(predictions, decimals)\n",
    "    \n",
    "    print(\"\\nPredicted Features:\")\n",
    "    for feature, value in formatted.items():\n",
    "        if isinstance(value, int):\n",
    "            print(f\"{feature:20s}: {value}\")\n",
    "        else:\n",
    "            print(f\"{feature:20s}: {value:.{decimals}f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0973f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|██████████| 1/1 [00:00<00:00, 167.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 1 hits, 0 misses (100.0% hit rate)\n",
      "Extracting MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features: 100%|██████████| 1/1 [00:00<00:00, 27.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'energy': 0.97,\n",
       " 'valence': 0.75,\n",
       " 'danceability': 0.49,\n",
       " 'loudness': -7.09,\n",
       " 'acousticness': 0.15,\n",
       " 'instrumentalness': 0.27,\n",
       " 'liveness': 0.22,\n",
       " 'speechiness': 0.07,\n",
       " 'key': 6,\n",
       " 'mode': 1,\n",
       " 'tempo_bins': 4}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and predict on new audio\n",
    "prediction1 = pipeline.predict(prc / 'audio/000010.mp3')\n",
    "format_predictions(prediction1, decimals =2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
