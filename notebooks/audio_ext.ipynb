{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37d85675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from src.music_recommender.config import Config\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from loguru import logger\n",
    "from scipy.signal import find_peaks\n",
    "import hashlib\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4287784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "asp = cfg.paths.processed / \"audio\"\n",
    "prc = cfg.paths.processed\n",
    "intr = cfg.paths.interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61fac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_sr: int = 22050,\n",
    "        new_ch: int = 1,\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        n_mels: int = 40,\n",
    "        target_duration: float = 30.0,\n",
    "        cache_dir: Path = intr / \"spectrogram_cache\",\n",
    "        save_cache: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.new_ch = new_ch\n",
    "        self.new_sr = new_sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_duration = target_duration\n",
    "        self.cache_dir = cache_dir\n",
    "        self.save_cache = save_cache\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _resample(\n",
    "        aud_sr: Tuple[np.ndarray, float], new_sr: int\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "        if sr == new_sr:\n",
    "            return aud, sr\n",
    "        if aud.ndim == 1:\n",
    "            res_aud = librosa.resample(aud, orig_sr=sr, target_sr=new_sr)\n",
    "        else:\n",
    "            res_aud = np.stack(\n",
    "                [\n",
    "                    librosa.resample(channel, orig_sr=sr, target_sr=new_sr)\n",
    "                    for channel in aud\n",
    "                ]\n",
    "            )\n",
    "        return res_aud, new_sr\n",
    "\n",
    "    @staticmethod\n",
    "    def _rechannel(\n",
    "        aud_sr: Tuple[np.ndarray, float], new_ch: int\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "        n_ch = 1 if aud.ndim == 1 else aud.shape[0]\n",
    "        if n_ch == new_ch:\n",
    "            return aud_sr\n",
    "        if new_ch == 1:\n",
    "            res_aud = np.mean(aud, axis=0, keepdims=True)\n",
    "            return res_aud, sr\n",
    "        if new_ch == 2:\n",
    "            res_aud = np.stack([aud, aud])\n",
    "            return res_aud, sr\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported number of channels: {new_ch}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_or_truncate(\n",
    "        aud_sr: Tuple[np.ndarray, float], target_duration: float\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "\n",
    "        target_samples = int(sr * target_duration)\n",
    "        current_samples = aud.shape[-1] if aud.ndim > 1 else len(aud)\n",
    "\n",
    "        if current_samples > target_samples:\n",
    "            if aud.ndim > 1:\n",
    "                return aud[:, :target_samples], sr\n",
    "            else:\n",
    "                return aud[:target_samples], sr\n",
    "        elif current_samples < target_samples:\n",
    "            pad_samples = target_samples - current_samples\n",
    "            if aud.ndim > 1:\n",
    "                pad_width = ((0, 0), (0, pad_samples))\n",
    "            else:\n",
    "                pad_width = (0, pad_samples)\n",
    "            return np.pad(aud, pad_width, mode=\"constant\", constant_values=0), sr\n",
    "        else:\n",
    "            return aud, sr\n",
    "\n",
    "    def _get_cache_path(self, audio_path: Path) -> Path:\n",
    "        params_hash = f\"{self.n_fft}_{self.hop_length}_{self.n_mels}_{self.new_sr}_{self.new_ch}_{self.target_duration}\"\n",
    "        return self.cache_dir / f\"{audio_path.stem}_{params_hash}.npz\"\n",
    "\n",
    "    def _load_spectr(\n",
    "        self,\n",
    "        audio_path: Path,\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        n_mels: int = 40,\n",
    "    ) -> Dict[str, Any]:\n",
    "        if self.save_cache:\n",
    "            cache_path = self._get_cache_path(audio_path)\n",
    "            if cache_path.exists():\n",
    "                cached = np.load(cache_path)\n",
    "                return {\n",
    "                    \"mel_spectrogram\": cached[\"mel_spectrogram\"],\n",
    "                    \"stft_spectrogram\": cached[\"stft_spectrogram\"],\n",
    "                    \"audio\": cached[\"audio\"],\n",
    "                    \"sr\": float(cached[\"sr\"]),\n",
    "                    \"path\": audio_path,\n",
    "                }\n",
    "        try:\n",
    "            aud_sr = librosa.load(audio_path)\n",
    "            aud_sr = self._pad_or_truncate(\n",
    "                aud_sr=aud_sr, target_duration=self.target_duration\n",
    "            )\n",
    "            aud_sr = self._rechannel(aud_sr=aud_sr, new_ch=self.new_ch)\n",
    "            aud_sr = self._resample(aud_sr=aud_sr, new_sr=self.new_sr)\n",
    "            aud, sr = aud_sr\n",
    "            \n",
    "            # Compute STFT for features that need it\n",
    "            stft = librosa.stft(aud, n_fft=n_fft, hop_length=hop_length)\n",
    "            stft_mag = np.abs(stft)\n",
    "            \n",
    "            # Compute mel spectrogram\n",
    "            mel_spect = librosa.feature.melspectrogram(\n",
    "                y=aud, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "            )\n",
    "            mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "            if self.save_cache:\n",
    "                cache_path = self._get_cache_path(audio_path)\n",
    "                np.savez_compressed(\n",
    "                    cache_path, \n",
    "                    mel_spectrogram=mel_spect_db, \n",
    "                    stft_spectrogram=stft_mag,\n",
    "                    audio=aud, \n",
    "                    sr=sr\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"mel_spectrogram\": mel_spect_db,\n",
    "                \"stft_spectrogram\": stft_mag,\n",
    "                \"audio\": aud,\n",
    "                \"sr\": sr,\n",
    "                \"path\": audio_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading audio from {audio_path}: {e}\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> np.ndarray:\n",
    "        results = []\n",
    "        for path in tqdm(X, desc=\"Loading spectrograms\"):\n",
    "            result = self._load_spectr(\n",
    "                path,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels,\n",
    "            )\n",
    "            results.append(result)\n",
    "        return np.array(results, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f073a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_title</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>album</th>\n",
       "      <th>year</th>\n",
       "      <th>release_date</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Freeway</td>\n",
       "      <td>Kurt Vile</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>Freeway</td>\n",
       "      <td>['Kurt Vile']</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-03-04</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.916</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.162</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>111.563</td>\n",
       "      <td>161173</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>Garbage and (Garbage and Fire)</td>\n",
       "      <td>Barnacled</td>\n",
       "      <td>6</td>\n",
       "      <td>Garbage and (garbage On Fire)</td>\n",
       "      <td>['Barnacled']</td>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.640</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.799</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>140.368</td>\n",
       "      <td>449467</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238</td>\n",
       "      <td>France Attacks</td>\n",
       "      <td>Barnacled</td>\n",
       "      <td>6</td>\n",
       "      <td>France Attacks</td>\n",
       "      <td>['Barnacled']</td>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.445</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>56.929</td>\n",
       "      <td>820707</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>CAVE</td>\n",
       "      <td>Butthash</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>['Cave']</td>\n",
       "      <td>Psychic Psummer</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-05-26</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.918</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>108.305</td>\n",
       "      <td>236960</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>459</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>CAVE</td>\n",
       "      <td>Butthash</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>['Cave']</td>\n",
       "      <td>Release</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-10-21</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.646</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.022</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>93.887</td>\n",
       "      <td>303680</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id                     track_title artist_name        album_title  \\\n",
       "0        10                         Freeway   Kurt Vile  Constant Hitmaker   \n",
       "1       237  Garbage and (Garbage and Fire)   Barnacled                  6   \n",
       "2       238                  France Attacks   Barnacled                  6   \n",
       "3       459            Machines and Muscles        CAVE           Butthash   \n",
       "4       459            Machines and Muscles        CAVE           Butthash   \n",
       "\n",
       "                            name        artists              album  year  \\\n",
       "0                        Freeway  ['Kurt Vile']  Constant Hitmaker  2008   \n",
       "1  Garbage and (garbage On Fire)  ['Barnacled']                  6  2003   \n",
       "2                 France Attacks  ['Barnacled']                  6  2003   \n",
       "3           Machines and Muscles       ['Cave']    Psychic Psummer  2009   \n",
       "4           Machines and Muscles       ['Cave']            Release  2014   \n",
       "\n",
       "  release_date  danceability  energy  key  loudness  mode  speechiness  \\\n",
       "0   2008-03-04         0.606   0.916    6    -8.162     1       0.0371   \n",
       "1   2003-01-01         0.280   0.640   11    -7.799     0       0.1230   \n",
       "2   2003-01-01         0.192   0.411    2    -9.445     1       0.0655   \n",
       "3   2009-05-26         0.584   0.918    7    -9.883     1       0.0345   \n",
       "4   2014-10-21         0.415   0.646    2   -12.022     1       0.0399   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  duration_ms  \\\n",
       "0        0.1400             0.356    0.1320   0.8890  111.563       161173   \n",
       "1        0.3490             0.675    0.1360   0.0537  140.368       449467   \n",
       "2        0.5390             0.709    0.0909   0.1390   56.929       820707   \n",
       "3        0.0254             0.770    0.3480   0.1140  108.305       236960   \n",
       "4        0.0189             0.948    0.0965   0.1230   93.887       303680   \n",
       "\n",
       "   time_signature  \n",
       "0             4.0  \n",
       "1             4.0  \n",
       "2             4.0  \n",
       "3             5.0  \n",
       "4             5.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data = pd.read_csv(prc / \"matched_metadata.csv\")\n",
    "audio_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccc510ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = audio_data[\"track_id\"].map(lambda id: asp / f\"{str(id).zfill(6)}.mp3\")\n",
    "y = audio_data[\n",
    "    [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bdf169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efb3ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aext = SpectrogramExtractor(save_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class StatsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr: float = 22050,\n",
    "        hop_length: int = 512,\n",
    "        n_fft: int = 2048,  \n",
    "        n_mfcc: int = 13,   \n",
    "        n_chroma: int = 12, \n",
    "        f0_min: float = 65.41,  \n",
    "        f0_max: float = 2093.0,\n",
    "        enable_cache: bool = True,\n",
    "        cache_path: Path = intr / \"stats_feat\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "        self.n_chroma = n_chroma\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.enable_cache = enable_cache\n",
    "        self.cache_path = cache_path\n",
    "        if self.enable_cache:\n",
    "            self.cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _get_cache_key(self, audio_path: str) -> str:\n",
    "        params_str = f\"{audio_path}_{self.sr}_{self.hop_length}_{self.n_fft}_{self.n_chroma}_{self.n_mfcc}_{self.f0_min}_{self.f0_max}\"\n",
    "        return hashlib.md5(params_str.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file(self, cache_key: str) -> Path:\n",
    "        return self.cache_path / f\"{cache_key}.pkl\"\n",
    "    \n",
    "    def _load_from_cache(self, cache_key: str):\n",
    "        cache_file = self._get_cache_file(cache_key)\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, cache_key: str, stats: dict):\n",
    "        \"\"\"Save features to cache\"\"\"\n",
    "        cache_file = self._get_cache_file(cache_key)\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(stats, f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def make_stats(feature_array, features_name):\n",
    "        return {\n",
    "            f\"{features_name}_mean\": np.mean(feature_array),\n",
    "            f\"{features_name}_std\": np.std(feature_array),\n",
    "            f\"{features_name}_min\": np.min(feature_array),\n",
    "            f\"{features_name}_max\": np.max(feature_array),\n",
    "            f\"{features_name}_median\": np.median(feature_array),\n",
    "            f\"{features_name}_q25\": np.percentile(feature_array, 25),\n",
    "            f\"{features_name}_q75\": np.percentile(feature_array, 75),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def temporal_extract(\n",
    "        audio: np.ndarray, stft_spectrogram: np.ndarray, sr: float, hop_length: int\n",
    "    ):\n",
    "\n",
    "        # Use STFT spectrogram for RMS\n",
    "        rms = librosa.feature.rms(S=stft_spectrogram, hop_length=hop_length)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=hop_length)\n",
    "\n",
    "        envelope = rms[0]\n",
    "        temporal_features = {}\n",
    "\n",
    "        if len(envelope) > 0 and np.max(envelope) > 0:\n",
    "            envelope_norm = envelope / np.max(envelope)\n",
    "\n",
    "            # Attack time (time to reach 90% of max amplitude)\n",
    "            attack_threshold = 0.9\n",
    "            attack_frame = np.argmax(envelope_norm >= attack_threshold)\n",
    "            temporal_features[\"attack_time\"] = (attack_frame * hop_length) / sr\n",
    "\n",
    "            # Temporal centroid (center of mass in time)\n",
    "            times = np.arange(len(envelope_norm)) * hop_length / sr\n",
    "            temporal_features[\"temporal_centroid\"] = np.sum(times * envelope_norm) / (\n",
    "                np.sum(envelope_norm) + 1e-8\n",
    "            )\n",
    "        else:\n",
    "            temporal_features[\"attack_time\"] = 0.0\n",
    "            temporal_features[\"temporal_centroid\"] = 0.0\n",
    "\n",
    "        frame_features = {\n",
    "            \"zcr\": zcr.flatten(),\n",
    "            \"rms\": rms.flatten(),\n",
    "        }\n",
    "\n",
    "        frame_features.update(temporal_features)\n",
    "        return frame_features\n",
    "        \n",
    "    @staticmethod\n",
    "    def spectral_extract(\n",
    "        audio: np.ndarray, stft_spectrogram: np.ndarray, sr: float, hop_length: int, n_mfcc: int\n",
    "    ):\n",
    "\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(\n",
    "            S=stft_spectrogram, hop_length=hop_length\n",
    "        )\n",
    "        spectral_flux = librosa.onset.onset_strength(S=stft_spectrogram, sr=sr)\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            S=librosa.power_to_db(stft_spectrogram**2),\n",
    "            sr=sr,\n",
    "            n_mfcc=n_mfcc,\n",
    "            hop_length=hop_length,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"spectral_centroid\": spectral_centroid.flatten(),\n",
    "            \"spectral_bandwidth\": spectral_bandwidth.flatten(),\n",
    "            \"spectral_rolloff\": spectral_rolloff.flatten(),\n",
    "            \"spectral_flatness\": spectral_flatness.flatten(),\n",
    "            \"spectral_flux\": spectral_flux,\n",
    "            \"mfccs\": mfccs,  # Shape: (13, n_frames)\n",
    "        }    \n",
    "    @staticmethod\n",
    "    def extract_rhythm_features(audio: np.ndarray, sr: float):\n",
    "        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        onset_env = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "\n",
    "        # Compute beat strength only if beats detected\n",
    "        beat_strength = np.mean(onset_env[beats]) if len(beats) > 0 else 0.0\n",
    "        onset_rate = len(beats) / (len(audio) / sr) if len(audio) > 0 else 0.0\n",
    "        if isinstance(tempo,np.ndarray):\n",
    "            tempo = float(np.median(tempo))\n",
    "        else:\n",
    "            tempo = float(tempo)\n",
    "        return {\n",
    "            \"tempo\": tempo,  # Scalar\n",
    "            \"beat_strength\": beat_strength,  # Scalar\n",
    "            \"onset_rate\": onset_rate,  # Scalar\n",
    "            \"onset_strength\": onset_env,  # Time series\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_chroma_features(audio: np.ndarray, sr: float, hop_length: int,n_chroma:int):\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr, hop_length=hop_length,n_chroma=n_chroma)\n",
    "\n",
    "        return {\n",
    "            \"chroma\": chroma,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_hpss_features(audio: np.ndarray):\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(audio)\n",
    "\n",
    "        h_energy = np.mean(y_harmonic**2)\n",
    "        p_energy = np.mean(y_percussive**2)\n",
    "\n",
    "        return {\n",
    "            \"harmonic_percussive_ratio\": h_energy / (p_energy + 1e-8),  # Scalar\n",
    "            \"harmonic_energy\": h_energy,  # Scalar\n",
    "            \"percussive_energy\": p_energy,  # Scalar\n",
    "        }, y_harmonic  # Return harmonic component for further analysis\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_harmonic_features(y_harmonic, sr, hop_length=512, n_fft=2048, f0_min=65.41, f0_max=2093.0):\n",
    "        features = {}\n",
    "\n",
    "        # Get fundamental frequency estimates\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "            y_harmonic,\n",
    "            fmin=f0_min,\n",
    "            fmax=f0_max,\n",
    "            sr=sr,\n",
    "            hop_length=hop_length,\n",
    "        )\n",
    "\n",
    "        # Compute STFT\n",
    "        stft = librosa.stft(y_harmonic, n_fft=n_fft, hop_length=hop_length)\n",
    "        mag_spec = np.abs(stft)\n",
    "        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "\n",
    "        # Use only voiced frames\n",
    "        valid_indices = np.where(voiced_flag)[0]\n",
    "        valid_f0 = f0[voiced_flag]\n",
    "\n",
    "        if len(valid_f0) > 0:\n",
    "            inharmonicity_values = []\n",
    "            t1_values = []\n",
    "            t2_values = []\n",
    "            t3_values = []\n",
    "\n",
    "            for idx, f0_val in zip(valid_indices, valid_f0):\n",
    "                if np.isnan(f0_val) or f0_val <= 0:\n",
    "                    continue\n",
    "\n",
    "                frame_spec = mag_spec[:, idx]\n",
    "\n",
    "                # --- INHARMONICITY CALCULATION ---\n",
    "                peaks, properties = find_peaks(\n",
    "                    frame_spec, height=np.max(frame_spec) * 0.1\n",
    "                )\n",
    "                peak_freqs = freqs[peaks]\n",
    "                peak_mags = properties[\"peak_heights\"]\n",
    "\n",
    "                if len(peak_freqs) > 0:\n",
    "                    sorted_idx = np.argsort(peak_mags)[::-1]\n",
    "                    peak_freqs = peak_freqs[sorted_idx[:10]]\n",
    "                    peak_mags = peak_mags[sorted_idx[:10]]\n",
    "\n",
    "                    deviations = []\n",
    "                    for n in range(1, min(8, len(peak_freqs) + 1)):\n",
    "                        expected_freq = f0_val * n\n",
    "                        if expected_freq < freqs[-1]:\n",
    "                            closest_idx = np.argmin(np.abs(peak_freqs - expected_freq))\n",
    "                            deviation = (\n",
    "                                np.abs(peak_freqs[closest_idx] - expected_freq)\n",
    "                                / expected_freq\n",
    "                            )\n",
    "                            deviations.append(deviation)\n",
    "\n",
    "                    if deviations:\n",
    "                        inharmonicity_values.append(np.mean(deviations))\n",
    "\n",
    "                # --- TRISTIMULUS CALCULATION ---\n",
    "                harmonic_energies = []\n",
    "                for n in range(1, 11):\n",
    "                    harmonic_freq = f0_val * n\n",
    "                    if harmonic_freq >= freqs[-1]:\n",
    "                        break\n",
    "\n",
    "                    bin_idx = np.argmin(np.abs(freqs - harmonic_freq))\n",
    "                    start_bin = max(0, bin_idx - 3)\n",
    "                    end_bin = min(len(frame_spec), bin_idx + 4)\n",
    "                    energy = np.sum(frame_spec[start_bin:end_bin])\n",
    "                    harmonic_energies.append(energy)\n",
    "\n",
    "                if len(harmonic_energies) >= 5:\n",
    "                    total_energy = np.sum(harmonic_energies) + 1e-8\n",
    "\n",
    "                    t1 = harmonic_energies[0] / total_energy\n",
    "                    t2 = np.sum(harmonic_energies[1:4]) / total_energy\n",
    "                    t3 = np.sum(harmonic_energies[4:]) / total_energy\n",
    "\n",
    "                    t1_values.append(t1)\n",
    "                    t2_values.append(t2)\n",
    "                    t3_values.append(t3)\n",
    "\n",
    "            # Aggregate features\n",
    "            features[\"inharmonicity\"] = (\n",
    "                np.mean(inharmonicity_values) if inharmonicity_values else 0.0\n",
    "            )\n",
    "            features[\"tristimulus_1\"] = np.mean(t1_values) if t1_values else 0.0\n",
    "            features[\"tristimulus_2\"] = np.mean(t2_values) if t2_values else 0.0\n",
    "            features[\"tristimulus_3\"] = np.mean(t3_values) if t3_values else 0.0\n",
    "            features[\"f0_mean\"] = np.mean(valid_f0)\n",
    "            features[\"f0_std\"] = np.std(valid_f0)\n",
    "            features[\"voiced_ratio\"] = np.sum(voiced_flag) / len(voiced_flag)\n",
    "        else:\n",
    "            # No voiced frames\n",
    "            features[\"inharmonicity\"] = 0.0\n",
    "            features[\"tristimulus_1\"] = 0.0\n",
    "            features[\"tristimulus_2\"] = 0.0\n",
    "            features[\"tristimulus_3\"] = 0.0\n",
    "            features[\"f0_mean\"] = 0.0\n",
    "            features[\"f0_std\"] = 0.0\n",
    "            features[\"voiced_ratio\"] = 0.0\n",
    "\n",
    "        return features\n",
    "    def extract_all_features(self, audio: np.ndarray, stft_spectrogram: np.ndarray):\n",
    "        all_features = {}\n",
    "\n",
    "        all_features.update(\n",
    "            self.temporal_extract(audio, stft_spectrogram, self.sr, self.hop_length)\n",
    "        )\n",
    "\n",
    "        all_features.update(\n",
    "            self.spectral_extract(audio, stft_spectrogram, self.sr, self.hop_length, self.n_mfcc)  # ← Pass n_mfcc\n",
    "        )\n",
    "\n",
    "        all_features.update(self.extract_rhythm_features(audio, self.sr))\n",
    "\n",
    "        all_features.update(\n",
    "            self.extract_chroma_features(audio, self.sr, self.hop_length,self.n_chroma)\n",
    "        )\n",
    "\n",
    "        hpss_features, y_harmonic = self.extract_hpss_features(audio)\n",
    "        all_features.update(hpss_features)\n",
    "\n",
    "        all_features.update(\n",
    "            self.extract_harmonic_features(y_harmonic, self.sr, self.hop_length, self.n_fft, self.f0_min, self.f0_max)  # ← Pass all params\n",
    "        )\n",
    "\n",
    "        return all_features\n",
    "    \n",
    "    def compute_statistics(self, features: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Convert all features to statistics (scalars)\"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        for key, val in features.items():\n",
    "            # Handle different feature types\n",
    "            if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "                # Already a scalar (tempo, attack_time, etc.)\n",
    "                stats[key] = float(val)\n",
    "\n",
    "            elif isinstance(val, np.ndarray):\n",
    "                if val.ndim == 1:\n",
    "                    # 1D time series (RMS, spectral_centroid, etc.)\n",
    "                    stats.update(self.make_stats(val, key))\n",
    "\n",
    "                elif val.ndim == 2:\n",
    "                    # 2D features (MFCCs, chroma)\n",
    "                    if key == \"mfccs\":\n",
    "                        # MFCCs: shape (13, n_frames)\n",
    "                        for i in range(val.shape[0]):  # Iterate over 13 coefficients\n",
    "                            mfcc_i = val[i, :]\n",
    "                            stats.update(self.make_stats(mfcc_i, f\"mfcc_{i}\"))\n",
    "\n",
    "                    elif key == \"chroma\":\n",
    "                        # Chroma: shape (12, n_frames)\n",
    "                        for i in range(val.shape[0]):  # Iterate over 12 pitch classes\n",
    "                            chroma_i = val[i, :]\n",
    "                            stats.update(self.make_stats(chroma_i, f\"chroma_{i}\"))\n",
    "\n",
    "                    else:\n",
    "                        # Generic 2D handling\n",
    "                        for i in range(val.shape[0]):\n",
    "                            stats.update(self.make_stats(val[i, :], f\"{key}_{i}\"))\n",
    "\n",
    "        return stats\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        feature_vectors = []\n",
    "\n",
    "        for item in tqdm(X, desc=\"Extracting features\"):\n",
    "            audio = item[\"audio\"]\n",
    "            stft_spectrogram = item[\"stft_spectrogram\"]  # Use STFT, not mel\n",
    "            audio_path = item[\"path\"]\n",
    "            \n",
    "            stats = None\n",
    "            \n",
    "            if self.enable_cache:\n",
    "                cache_key = self._get_cache_key(audio_path)\n",
    "                stats = self._load_from_cache(cache_key)\n",
    "            \n",
    "            if stats is None:\n",
    "                features = self.extract_all_features(audio, stft_spectrogram)\n",
    "                stats = self.compute_statistics(features)\n",
    "                \n",
    "                if self.enable_cache:\n",
    "                    cache_key = self._get_cache_key(audio_path)\n",
    "                    self._save_to_cache(cache_key, stats)\n",
    "            \n",
    "            feature_vectors.append(stats)\n",
    "\n",
    "        return pd.DataFrame(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dc10faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms:  32%|███▏      | 493/1556 [02:23<05:09,  3.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      6\u001b[39m extraction_pipeline = Pipeline([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mspectrogram\u001b[39m\u001b[33m'\u001b[39m, SpectrogramExtractor(\n\u001b[32m      8\u001b[39m         new_sr=\u001b[32m22050\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     ))\n\u001b[32m     15\u001b[39m ])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X_train_extracted = \u001b[43mextraction_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m X_test_extracted = extraction_pipeline.transform(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:719\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[32m    681\u001b[39m \n\u001b[32m    682\u001b[39m \u001b[33;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    716\u001b[39m \u001b[33;03m    Transformed samples.\u001b[39;00m\n\u001b[32m    717\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    718\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m last_step = \u001b[38;5;28mself\u001b[39m._final_estimator\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mSpectrogramExtractor.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    148\u001b[39m results = []\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm(X, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading spectrograms\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_spectr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     results.append(result)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(results, dtype=\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mSpectrogramExtractor._load_spectr\u001b[39m\u001b[34m(self, audio_path, n_fft, hop_length, n_mels)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_cache:\n\u001b[32m    125\u001b[39m         cache_path = \u001b[38;5;28mself\u001b[39m._get_cache_path(audio_path)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msavez_compressed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmel_spectrogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmel_spect_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstft_spectrogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstft_mag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m            \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maud\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmel_spectrogram\u001b[39m\u001b[33m\"\u001b[39m: mel_spect_db,\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstft_spectrogram\u001b[39m\u001b[33m\"\u001b[39m: stft_mag,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: audio_path\n\u001b[32m    140\u001b[39m     }\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:766\u001b[39m, in \u001b[36msavez_compressed\u001b[39m\u001b[34m(file, allow_pickle, *args, **kwds)\u001b[39m\n\u001b[32m    694\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_savez_compressed_dispatcher)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msavez_compressed\u001b[39m(file, *args, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwds):\n\u001b[32m    696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[33;03m    Save several arrays into a single file in compressed ``.npz`` format.\u001b[39;00m\n\u001b[32m    698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    764\u001b[39m \n\u001b[32m    765\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[43m_savez\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:799\u001b[39m, in \u001b[36m_savez\u001b[39m\u001b[34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m         \u001b[38;5;66;03m# always force zip64, gh-10776\u001b[39;00m\n\u001b[32m    798\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m zipf.open(fname, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, force_zip64=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m             \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    803\u001b[39m     zipf.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_format_impl.py:777\u001b[39m, in \u001b[36mwrite_array\u001b[39m\u001b[34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy.nditer(\n\u001b[32m    775\u001b[39m             array, flags=[\u001b[33m'\u001b[39m\u001b[33mexternal_loop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbuffered\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mzerosize_ok\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    776\u001b[39m             buffersize=buffersize, order=\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m         \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/lib/python3.11/zipfile.py:1169\u001b[39m, in \u001b[36m_ZipWriteFile.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28mself\u001b[39m._crc = crc32(data, \u001b[38;5;28mself\u001b[39m._crc)\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compressor:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28mself\u001b[39m._compress_size += \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._fileobj.write(data)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Basic usage - extract features and train a model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Extraction pipeline\n",
    "extraction_pipeline = Pipeline([\n",
    "    ('spectrogram', SpectrogramExtractor(\n",
    "        new_sr=22050,\n",
    "        save_cache=True\n",
    "    )),\n",
    "    ('features', StatsFeatureExtractor(\n",
    "        sr=22050,\n",
    "        enable_cache=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Extract features\n",
    "X_train_extracted = extraction_pipeline.fit_transform(X_train)\n",
    "X_test_extracted = extraction_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2741f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b69943",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- tempo\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model.fit(X_train_extracted, y_train)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_extracted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_squared_error(y_test,\u001b[38;5;250m \u001b[39mpredictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:1065\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1063\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1068\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- tempo\n"
     ]
    }
   ],
   "source": [
    "# Train model on extracted features\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train_extracted, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_extracted)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"R²: {r2_score(y_test, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
