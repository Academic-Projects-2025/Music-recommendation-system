{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d85675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from src.music_recommender.config import Config\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from loguru import logger\n",
    "from scipy.signal import find_peaks\n",
    "import hashlib\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4287784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "asp = cfg.paths.processed / \"audio\"\n",
    "prc = cfg.paths.processed\n",
    "intr = cfg.paths.interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61fac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_sr: int = 16000,\n",
    "        new_ch: int = 1,\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        n_mels: int = 40,\n",
    "        target_duration: float = 30.0,\n",
    "        cache_dir: Path = intr / \"spectrogram_cache\",\n",
    "        save_cache: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.new_ch = new_ch\n",
    "        self.new_sr = new_sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_duration = target_duration\n",
    "        self.cache_dir = cache_dir\n",
    "        self.save_cache = save_cache\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _resample(\n",
    "        aud_sr: Tuple[np.ndarray, float], new_sr: int\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "        if sr == new_sr:\n",
    "            return aud, sr\n",
    "        if aud.ndim == 1:\n",
    "            res_aud = librosa.resample(aud, orig_sr=sr, target_sr=new_sr)\n",
    "        else:\n",
    "            res_aud = np.stack(\n",
    "                [\n",
    "                    librosa.resample(channel, orig_sr=sr, target_sr=new_sr)\n",
    "                    for channel in aud\n",
    "                ]\n",
    "            )\n",
    "        return res_aud, new_sr\n",
    "\n",
    "    @staticmethod\n",
    "    def _rechannel(\n",
    "        aud_sr: Tuple[np.ndarray, float], new_ch: int\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "        n_ch = 1 if aud.ndim == 1 else aud.shape[0]\n",
    "        if n_ch == new_ch:\n",
    "            return aud_sr\n",
    "        if new_ch == 1:\n",
    "            res_aud = np.mean(aud, axis=0, keepdims=True)\n",
    "            return res_aud, sr\n",
    "        if new_ch == 2:\n",
    "            res_aud = np.stack([aud, aud])\n",
    "            return res_aud, sr\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported number of channels: {new_ch}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_or_truncate(\n",
    "        aud_sr: Tuple[np.ndarray, float], target_duration: float\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        aud, sr = aud_sr\n",
    "\n",
    "        target_samples = int(sr * target_duration)\n",
    "        current_samples = aud.shape[-1] if aud.ndim > 1 else len(aud)\n",
    "\n",
    "        if current_samples > target_samples:\n",
    "            if aud.ndim > 1:\n",
    "                return aud[:, :target_samples], sr\n",
    "            else:\n",
    "                return aud[:target_samples], sr\n",
    "        elif current_samples < target_samples:\n",
    "            pad_samples = target_samples - current_samples\n",
    "            if aud.ndim > 1:\n",
    "                pad_width = ((0, 0), (0, pad_samples))\n",
    "            else:\n",
    "                pad_width = (0, pad_samples)\n",
    "            return np.pad(aud, pad_width, mode=\"constant\", constant_values=0), sr\n",
    "        else:\n",
    "            return aud, sr\n",
    "\n",
    "    def _get_cache_path(self, audio_path: Path) -> Path:\n",
    "        params_hash = f\"{self.n_fft}_{self.hop_length}_{self.n_mels}_{self.new_sr}_{self.new_ch}_{self.target_duration}\"\n",
    "        return self.cache_dir / f\"{audio_path.stem}_{params_hash}.npz\"\n",
    "\n",
    "    def _load_spectr(\n",
    "        self,\n",
    "        audio_path: Path,\n",
    "        n_fft: int = 2048,\n",
    "        hop_length: int = 512,\n",
    "        n_mels: int = 40,\n",
    "    ) -> Dict[str, Any]:\n",
    "        if self.save_cache:\n",
    "            cache_path = self._get_cache_path(audio_path)\n",
    "            if cache_path.exists():\n",
    "                cached = np.load(cache_path)\n",
    "                return {\n",
    "                    \"mel_spectrogram\": cached[\"mel_spectrogram\"],\n",
    "                    \"stft_spectrogram\": cached[\"stft_spectrogram\"],\n",
    "                    \"audio\": cached[\"audio\"],\n",
    "                    \"sr\": float(cached[\"sr\"]),\n",
    "                    \"path\": audio_path,\n",
    "                }\n",
    "        try:\n",
    "            aud_sr = librosa.load(audio_path)\n",
    "            aud_sr = self._pad_or_truncate(\n",
    "                aud_sr=aud_sr, target_duration=self.target_duration\n",
    "            )\n",
    "            aud_sr = self._rechannel(aud_sr=aud_sr, new_ch=self.new_ch)\n",
    "            aud_sr = self._resample(aud_sr=aud_sr, new_sr=self.new_sr)\n",
    "            aud, sr = aud_sr\n",
    "            \n",
    "            # Compute STFT for features that need it\n",
    "            stft = librosa.stft(aud, n_fft=n_fft, hop_length=hop_length)\n",
    "            stft_mag = np.abs(stft)\n",
    "            \n",
    "            # Compute mel spectrogram\n",
    "            mel_spect = librosa.feature.melspectrogram(\n",
    "                y=aud, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "            )\n",
    "            mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "            if self.save_cache:\n",
    "                cache_path = self._get_cache_path(audio_path)\n",
    "                np.savez_compressed(\n",
    "                    cache_path, \n",
    "                    mel_spectrogram=mel_spect_db, \n",
    "                    stft_spectrogram=stft_mag,\n",
    "                    audio=aud, \n",
    "                    sr=sr\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"mel_spectrogram\": mel_spect_db,\n",
    "                \"stft_spectrogram\": stft_mag,\n",
    "                \"audio\": aud,\n",
    "                \"sr\": sr,\n",
    "                \"path\": audio_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading audio from {audio_path}: {e}\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> np.ndarray:\n",
    "        results = []\n",
    "        for path in tqdm(X, desc=\"Loading spectrograms\"):\n",
    "            result = self._load_spectr(\n",
    "                path,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels,\n",
    "            )\n",
    "            results.append(result)\n",
    "        return np.array(results, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f073a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_title</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>album</th>\n",
       "      <th>year</th>\n",
       "      <th>release_date</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Freeway</td>\n",
       "      <td>Kurt Vile</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>Freeway</td>\n",
       "      <td>['Kurt Vile']</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-03-04</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.916</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.162</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>111.563</td>\n",
       "      <td>161173</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>Garbage and (Garbage and Fire)</td>\n",
       "      <td>Barnacled</td>\n",
       "      <td>6</td>\n",
       "      <td>Garbage and (garbage On Fire)</td>\n",
       "      <td>['Barnacled']</td>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.640</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.799</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>140.368</td>\n",
       "      <td>449467</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238</td>\n",
       "      <td>France Attacks</td>\n",
       "      <td>Barnacled</td>\n",
       "      <td>6</td>\n",
       "      <td>France Attacks</td>\n",
       "      <td>['Barnacled']</td>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.445</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>56.929</td>\n",
       "      <td>820707</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>CAVE</td>\n",
       "      <td>Butthash</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>['Cave']</td>\n",
       "      <td>Psychic Psummer</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009-05-26</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.918</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>108.305</td>\n",
       "      <td>236960</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>459</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>CAVE</td>\n",
       "      <td>Butthash</td>\n",
       "      <td>Machines and Muscles</td>\n",
       "      <td>['Cave']</td>\n",
       "      <td>Release</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-10-21</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.646</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.022</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>93.887</td>\n",
       "      <td>303680</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id                     track_title artist_name        album_title  \\\n",
       "0        10                         Freeway   Kurt Vile  Constant Hitmaker   \n",
       "1       237  Garbage and (Garbage and Fire)   Barnacled                  6   \n",
       "2       238                  France Attacks   Barnacled                  6   \n",
       "3       459            Machines and Muscles        CAVE           Butthash   \n",
       "4       459            Machines and Muscles        CAVE           Butthash   \n",
       "\n",
       "                            name        artists              album  year  \\\n",
       "0                        Freeway  ['Kurt Vile']  Constant Hitmaker  2008   \n",
       "1  Garbage and (garbage On Fire)  ['Barnacled']                  6  2003   \n",
       "2                 France Attacks  ['Barnacled']                  6  2003   \n",
       "3           Machines and Muscles       ['Cave']    Psychic Psummer  2009   \n",
       "4           Machines and Muscles       ['Cave']            Release  2014   \n",
       "\n",
       "  release_date  danceability  energy  key  loudness  mode  speechiness  \\\n",
       "0   2008-03-04         0.606   0.916    6    -8.162     1       0.0371   \n",
       "1   2003-01-01         0.280   0.640   11    -7.799     0       0.1230   \n",
       "2   2003-01-01         0.192   0.411    2    -9.445     1       0.0655   \n",
       "3   2009-05-26         0.584   0.918    7    -9.883     1       0.0345   \n",
       "4   2014-10-21         0.415   0.646    2   -12.022     1       0.0399   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  duration_ms  \\\n",
       "0        0.1400             0.356    0.1320   0.8890  111.563       161173   \n",
       "1        0.3490             0.675    0.1360   0.0537  140.368       449467   \n",
       "2        0.5390             0.709    0.0909   0.1390   56.929       820707   \n",
       "3        0.0254             0.770    0.3480   0.1140  108.305       236960   \n",
       "4        0.0189             0.948    0.0965   0.1230   93.887       303680   \n",
       "\n",
       "   time_signature  \n",
       "0             4.0  \n",
       "1             4.0  \n",
       "2             4.0  \n",
       "3             5.0  \n",
       "4             5.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data = pd.read_csv(prc / \"matched_metadata.csv\")\n",
    "audio_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccc510ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = audio_data[\"track_id\"].map(lambda id: asp / f\"{str(id).zfill(6)}.mp3\")\n",
    "y = audio_data[\n",
    "    [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bdf169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efb3ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aext = SpectrogramExtractor(save_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2716e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms: 100%|██████████| 1556/1556 [00:43<00:00, 35.47it/s]\n",
      "Loading spectrograms: 100%|██████████| 173/173 [00:06<00:00, 26.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_features = aext.fit_transform(X_train)\n",
    "X_test_features = aext.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18bb316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class StatsFeatureExtractor(BaseEstimator, TransformerMixin):  # Fixed typo\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr: float = 22050,\n",
    "        hop_length: int = 512,\n",
    "        enable_cache: bool = True,\n",
    "        cache_path: Path = intr / \"stats_feat\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.enable_cache = enable_cache\n",
    "        self.cache_path = cache_path\n",
    "        if self.enable_cache:\n",
    "            self.cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _get_cache_key(self, audio_path: str) -> str:\n",
    "        return hashlib.md5(str(audio_path).encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file(self, cache_key: str) -> Path:\n",
    "        return self.cache_path / f\"{cache_key}.pkl\"\n",
    "    \n",
    "    def _load_from_cache(self, cache_key: str):\n",
    "        cache_file = self._get_cache_file(cache_key)\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, cache_key: str, stats: dict):\n",
    "        \"\"\"Save features to cache\"\"\"\n",
    "        cache_file = self._get_cache_file(cache_key)\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(stats, f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def make_stats(feature_array, features_name):\n",
    "        return {\n",
    "            f\"{features_name}_mean\": np.mean(feature_array),\n",
    "            f\"{features_name}_std\": np.std(feature_array),\n",
    "            f\"{features_name}_min\": np.min(feature_array),\n",
    "            f\"{features_name}_max\": np.max(feature_array),\n",
    "            f\"{features_name}_median\": np.median(feature_array),\n",
    "            f\"{features_name}_q25\": np.percentile(feature_array, 25),\n",
    "            f\"{features_name}_q75\": np.percentile(feature_array, 75),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def temporal_extract(\n",
    "        audio: np.ndarray, stft_spectrogram: np.ndarray, sr: float, hop_length: int\n",
    "    ):\n",
    "\n",
    "        # Use STFT spectrogram for RMS\n",
    "        rms = librosa.feature.rms(S=stft_spectrogram, hop_length=hop_length)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=hop_length)\n",
    "\n",
    "        envelope = rms[0]\n",
    "        temporal_features = {}\n",
    "\n",
    "        if len(envelope) > 0 and np.max(envelope) > 0:\n",
    "            envelope_norm = envelope / np.max(envelope)\n",
    "\n",
    "            # Attack time (time to reach 90% of max amplitude)\n",
    "            attack_threshold = 0.9\n",
    "            attack_frame = np.argmax(envelope_norm >= attack_threshold)\n",
    "            temporal_features[\"attack_time\"] = (attack_frame * hop_length) / sr\n",
    "\n",
    "            # Temporal centroid (center of mass in time)\n",
    "            times = np.arange(len(envelope_norm)) * hop_length / sr\n",
    "            temporal_features[\"temporal_centroid\"] = np.sum(times * envelope_norm) / (\n",
    "                np.sum(envelope_norm) + 1e-8\n",
    "            )\n",
    "        else:\n",
    "            temporal_features[\"attack_time\"] = 0.0\n",
    "            temporal_features[\"temporal_centroid\"] = 0.0\n",
    "\n",
    "        frame_features = {\n",
    "            \"zcr\": zcr.flatten(),\n",
    "            \"rms\": rms.flatten(),\n",
    "        }\n",
    "\n",
    "        frame_features.update(temporal_features)\n",
    "        return frame_features\n",
    "        \n",
    "    @staticmethod\n",
    "    def spectral_extract(\n",
    "        audio: np.ndarray, stft_spectrogram: np.ndarray, sr: float, hop_length: int\n",
    "    ):\n",
    "\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(\n",
    "            S=stft_spectrogram, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(\n",
    "            S=stft_spectrogram, hop_length=hop_length\n",
    "        )\n",
    "        spectral_flux = librosa.onset.onset_strength(S=stft_spectrogram, sr=sr)\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            S=librosa.power_to_db(stft_spectrogram**2),\n",
    "            sr=sr,\n",
    "            n_mfcc=13,\n",
    "            hop_length=hop_length,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"spectral_centroid\": spectral_centroid.flatten(),\n",
    "            \"spectral_bandwidth\": spectral_bandwidth.flatten(),\n",
    "            \"spectral_rolloff\": spectral_rolloff.flatten(),\n",
    "            \"spectral_flatness\": spectral_flatness.flatten(),\n",
    "            \"spectral_flux\": spectral_flux,\n",
    "            \"mfccs\": mfccs,  # Shape: (13, n_frames)\n",
    "        }    \n",
    "    @staticmethod\n",
    "    def extract_rhythm_features(audio: np.ndarray, sr: float):\n",
    "        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        onset_env = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "\n",
    "        # Compute beat strength only if beats detected\n",
    "        beat_strength = np.mean(onset_env[beats]) if len(beats) > 0 else 0.0\n",
    "        onset_rate = len(beats) / (len(audio) / sr) if len(audio) > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            \"tempo\": tempo,  # Scalar\n",
    "            \"beat_strength\": beat_strength,  # Scalar\n",
    "            \"onset_rate\": onset_rate,  # Scalar\n",
    "            \"onset_strength\": onset_env,  # Time series\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_chroma_features(audio: np.ndarray, sr: float, hop_length: int):\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr, hop_length=hop_length)\n",
    "\n",
    "        return {\n",
    "            \"chroma\": chroma,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_hpss_features(audio: np.ndarray):\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(audio)\n",
    "\n",
    "        h_energy = np.mean(y_harmonic**2)\n",
    "        p_energy = np.mean(y_percussive**2)\n",
    "\n",
    "        return {\n",
    "            \"harmonic_percussive_ratio\": h_energy / (p_energy + 1e-8),  # Scalar\n",
    "            \"harmonic_energy\": h_energy,  # Scalar\n",
    "            \"percussive_energy\": p_energy,  # Scalar\n",
    "        }, y_harmonic  # Return harmonic component for further analysis\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_harmonic_features(y_harmonic, sr, hop_length=512, n_fft=2048):\n",
    "        features = {}\n",
    "\n",
    "        # Get fundamental frequency estimates\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "            y_harmonic,\n",
    "            fmin=librosa.note_to_hz(\"C2\"),\n",
    "            fmax=librosa.note_to_hz(\"C7\"),\n",
    "            sr=sr,\n",
    "            hop_length=hop_length,\n",
    "        )\n",
    "\n",
    "        # Compute STFT\n",
    "        stft = librosa.stft(y_harmonic, n_fft=n_fft, hop_length=hop_length)\n",
    "        mag_spec = np.abs(stft)\n",
    "        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "\n",
    "        # Use only voiced frames\n",
    "        valid_indices = np.where(voiced_flag)[0]\n",
    "        valid_f0 = f0[voiced_flag]\n",
    "\n",
    "        if len(valid_f0) > 0:\n",
    "            inharmonicity_values = []\n",
    "            t1_values = []\n",
    "            t2_values = []\n",
    "            t3_values = []\n",
    "\n",
    "            for idx, f0_val in zip(valid_indices, valid_f0):\n",
    "                if np.isnan(f0_val) or f0_val <= 0:\n",
    "                    continue\n",
    "\n",
    "                frame_spec = mag_spec[:, idx]\n",
    "\n",
    "                # --- INHARMONICITY CALCULATION ---\n",
    "                peaks, properties = find_peaks(\n",
    "                    frame_spec, height=np.max(frame_spec) * 0.1\n",
    "                )\n",
    "                peak_freqs = freqs[peaks]\n",
    "                peak_mags = properties[\"peak_heights\"]\n",
    "\n",
    "                if len(peak_freqs) > 0:\n",
    "                    sorted_idx = np.argsort(peak_mags)[::-1]\n",
    "                    peak_freqs = peak_freqs[sorted_idx[:10]]\n",
    "                    peak_mags = peak_mags[sorted_idx[:10]]\n",
    "\n",
    "                    deviations = []\n",
    "                    for n in range(1, min(8, len(peak_freqs) + 1)):\n",
    "                        expected_freq = f0_val * n\n",
    "                        if expected_freq < freqs[-1]:\n",
    "                            closest_idx = np.argmin(np.abs(peak_freqs - expected_freq))\n",
    "                            deviation = (\n",
    "                                np.abs(peak_freqs[closest_idx] - expected_freq)\n",
    "                                / expected_freq\n",
    "                            )\n",
    "                            deviations.append(deviation)\n",
    "\n",
    "                    if deviations:\n",
    "                        inharmonicity_values.append(np.mean(deviations))\n",
    "\n",
    "                # --- TRISTIMULUS CALCULATION ---\n",
    "                harmonic_energies = []\n",
    "                for n in range(1, 11):\n",
    "                    harmonic_freq = f0_val * n\n",
    "                    if harmonic_freq >= freqs[-1]:\n",
    "                        break\n",
    "\n",
    "                    bin_idx = np.argmin(np.abs(freqs - harmonic_freq))\n",
    "                    start_bin = max(0, bin_idx - 3)\n",
    "                    end_bin = min(len(frame_spec), bin_idx + 4)\n",
    "                    energy = np.sum(frame_spec[start_bin:end_bin])\n",
    "                    harmonic_energies.append(energy)\n",
    "\n",
    "                if len(harmonic_energies) >= 5:\n",
    "                    total_energy = np.sum(harmonic_energies) + 1e-8\n",
    "\n",
    "                    t1 = harmonic_energies[0] / total_energy\n",
    "                    t2 = np.sum(harmonic_energies[1:4]) / total_energy\n",
    "                    t3 = np.sum(harmonic_energies[4:]) / total_energy\n",
    "\n",
    "                    t1_values.append(t1)\n",
    "                    t2_values.append(t2)\n",
    "                    t3_values.append(t3)\n",
    "\n",
    "            # Aggregate features\n",
    "            features[\"inharmonicity\"] = (\n",
    "                np.mean(inharmonicity_values) if inharmonicity_values else 0.0\n",
    "            )\n",
    "            features[\"tristimulus_1\"] = np.mean(t1_values) if t1_values else 0.0\n",
    "            features[\"tristimulus_2\"] = np.mean(t2_values) if t2_values else 0.0\n",
    "            features[\"tristimulus_3\"] = np.mean(t3_values) if t3_values else 0.0\n",
    "            features[\"f0_mean\"] = np.mean(valid_f0)\n",
    "            features[\"f0_std\"] = np.std(valid_f0)\n",
    "            features[\"voiced_ratio\"] = np.sum(voiced_flag) / len(voiced_flag)\n",
    "        else:\n",
    "            # No voiced frames\n",
    "            features[\"inharmonicity\"] = 0.0\n",
    "            features[\"tristimulus_1\"] = 0.0\n",
    "            features[\"tristimulus_2\"] = 0.0\n",
    "            features[\"tristimulus_3\"] = 0.0\n",
    "            features[\"f0_mean\"] = 0.0\n",
    "            features[\"f0_std\"] = 0.0\n",
    "            features[\"voiced_ratio\"] = 0.0\n",
    "\n",
    "        return features\n",
    "    def extract_all_features(self, audio: np.ndarray, stft_spectrogram: np.ndarray):\n",
    "        \"\"\"\n",
    "        Extract all features from audio\n",
    "        \n",
    "        Args:\n",
    "            audio: Raw audio signal\n",
    "            stft_spectrogram: STFT magnitude spectrogram\n",
    "        \"\"\"\n",
    "        all_features = {}\n",
    "\n",
    "        all_features.update(\n",
    "            self.temporal_extract(audio, stft_spectrogram, self.sr, self.hop_length)\n",
    "        )\n",
    "\n",
    "        all_features.update(\n",
    "            self.spectral_extract(audio, stft_spectrogram, self.sr, self.hop_length)\n",
    "        )\n",
    "\n",
    "        all_features.update(self.extract_rhythm_features(audio, self.sr))\n",
    "\n",
    "        all_features.update(\n",
    "            self.extract_chroma_features(audio, self.sr, self.hop_length)\n",
    "        )\n",
    "\n",
    "        hpss_features, y_harmonic = self.extract_hpss_features(audio)\n",
    "        all_features.update(hpss_features)\n",
    "\n",
    "        all_features.update(\n",
    "            self.extract_harmonic_features(y_harmonic, self.sr, self.hop_length)\n",
    "        )\n",
    "\n",
    "        return all_features\n",
    "    def compute_statistics(self, features: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Convert all features to statistics (scalars)\"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        for key, val in features.items():\n",
    "            # Handle different feature types\n",
    "            if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "                # Already a scalar (tempo, attack_time, etc.)\n",
    "                stats[key] = float(val)\n",
    "\n",
    "            elif isinstance(val, np.ndarray):\n",
    "                if val.ndim == 1:\n",
    "                    # 1D time series (RMS, spectral_centroid, etc.)\n",
    "                    stats.update(self.make_stats(val, key))\n",
    "\n",
    "                elif val.ndim == 2:\n",
    "                    # 2D features (MFCCs, chroma)\n",
    "                    if key == \"mfccs\":\n",
    "                        # MFCCs: shape (13, n_frames)\n",
    "                        for i in range(val.shape[0]):  # Iterate over 13 coefficients\n",
    "                            mfcc_i = val[i, :]\n",
    "                            stats.update(self.make_stats(mfcc_i, f\"mfcc_{i}\"))\n",
    "\n",
    "                    elif key == \"chroma\":\n",
    "                        # Chroma: shape (12, n_frames)\n",
    "                        for i in range(val.shape[0]):  # Iterate over 12 pitch classes\n",
    "                            chroma_i = val[i, :]\n",
    "                            stats.update(self.make_stats(chroma_i, f\"chroma_{i}\"))\n",
    "\n",
    "                    else:\n",
    "                        # Generic 2D handling\n",
    "                        for i in range(val.shape[0]):\n",
    "                            stats.update(self.make_stats(val[i, :], f\"{key}_{i}\"))\n",
    "\n",
    "        return stats\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        feature_vectors = []\n",
    "\n",
    "        for item in tqdm(X, desc=\"Extracting features\"):\n",
    "            audio = item[\"audio\"]\n",
    "            stft_spectrogram = item[\"stft_spectrogram\"]  # Use STFT, not mel\n",
    "            audio_path = item[\"path\"]\n",
    "            \n",
    "            stats = None\n",
    "            \n",
    "            if self.enable_cache:\n",
    "                cache_key = self._get_cache_key(audio_path)\n",
    "                stats = self._load_from_cache(cache_key)\n",
    "            \n",
    "            if stats is None:\n",
    "                features = self.extract_all_features(audio, stft_spectrogram)\n",
    "                stats = self.compute_statistics(features)\n",
    "                \n",
    "                if self.enable_cache:\n",
    "                    cache_key = self._get_cache_key(audio_path)\n",
    "                    self._save_to_cache(cache_key, stats)\n",
    "            \n",
    "            feature_vectors.append(stats)\n",
    "\n",
    "        return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dc10faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms: 100%|██████████| 1556/1556 [01:16<00:00, 20.25it/s]\n",
      "Extracting features:   1%|          | 16/1556 [01:05<1:44:52,  4.09s/it]\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "CPUDispatcher(<function _viterbi at 0x72ab9cf49ee0>) returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numba/core/serialize.py:30\u001b[39m, in \u001b[36m_numba_unpickle\u001b[39m\u001b[34m(address, bytedata, hashed)\u001b[39m\n\u001b[32m     27\u001b[39m _unpickled_memo = {}\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m        unpickled object\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mSystemError\u001b[39m: <function _numba_unpickle at 0x72ab9e09af20> returned a result with an exception set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      6\u001b[39m extraction_pipeline = Pipeline([\n\u001b[32m      7\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mspectrogram\u001b[39m\u001b[33m'\u001b[39m, SpectrogramExtractor(\n\u001b[32m      8\u001b[39m         new_sr=\u001b[32m16000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     ))\n\u001b[32m     15\u001b[39m ])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X_train_extracted = \u001b[43mextraction_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m X_test_extracted = extraction_pipeline.transform(X_test)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train model on extracted features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:731\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    725\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    726\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    727\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    728\u001b[39m     all_params=params,\n\u001b[32m    729\u001b[39m )\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    736\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    737\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 354\u001b[39m, in \u001b[36mStatsFeatureExtractor.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    351\u001b[39m     stats = \u001b[38;5;28mself\u001b[39m._load_from_cache(cache_key)\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstft_spectrogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     stats = \u001b[38;5;28mself\u001b[39m.compute_statistics(features)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_cache:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mStatsFeatureExtractor.extract_all_features\u001b[39m\u001b[34m(self, audio, stft_spectrogram)\u001b[39m\n\u001b[32m    294\u001b[39m hpss_features, y_harmonic = \u001b[38;5;28mself\u001b[39m.extract_hpss_features(audio)\n\u001b[32m    295\u001b[39m all_features.update(hpss_features)\n\u001b[32m    297\u001b[39m all_features.update(\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_harmonic_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_harmonic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_features\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mStatsFeatureExtractor.extract_harmonic_features\u001b[39m\u001b[34m(y_harmonic, sr, hop_length, n_fft)\u001b[39m\n\u001b[32m    167\u001b[39m features = {}\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Get fundamental frequency estimates\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m f0, voiced_flag, voiced_probs = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpyin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_harmonic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnote_to_hz\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnote_to_hz\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC7\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Compute STFT\u001b[39;00m\n\u001b[32m    179\u001b[39m stft = librosa.stft(y_harmonic, n_fft=n_fft, hop_length=hop_length)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:883\u001b[39m, in \u001b[36mpyin\u001b[39m\u001b[34m(y, fmin, fmax, sr, frame_length, win_length, hop_length, n_thresholds, beta_parameters, boltzmann_parameter, resolution, max_transition_rate, switch_prob, no_trough_prob, fill_na, center, pad_mode)\u001b[39m\n\u001b[32m    879\u001b[39m transition = np.kron(t_switch, transition)\n\u001b[32m    881\u001b[39m p_init = np.ones(\u001b[32m2\u001b[39m * n_pitch_bins) / (\u001b[32m2\u001b[39m * n_pitch_bins)\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m states = \u001b[43msequence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mviterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Find f0 corresponding to each decoded pitch bin.\u001b[39;00m\n\u001b[32m    886\u001b[39m freqs = fmin * \u001b[32m2\u001b[39m ** (np.arange(n_pitch_bins) / (\u001b[32m12\u001b[39m * n_bins_per_semitone))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/librosa/sequence.py:1318\u001b[39m, in \u001b[36mviterbi\u001b[39m\u001b[34m(prob, transition, p_init, return_logp)\u001b[39m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;66;03m# Vectorize the helper\u001b[39;00m\n\u001b[32m   1314\u001b[39m     __viterbi = np.vectorize(\n\u001b[32m   1315\u001b[39m         _helper, otypes=[np.uint16, np.float64], signature=\u001b[33m\"\u001b[39m\u001b[33m(s,t)->(t),(1)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1316\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1318\u001b[39m     states, logp = \u001b[43m__viterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m     \u001b[38;5;66;03m# Flatten out the trailing dimension introduced by vectorization\u001b[39;00m\n\u001b[32m   1321\u001b[39m     logp = logp[..., \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:2544\u001b[39m, in \u001b[36mvectorize.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2541\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_stage_2(*args, **kwargs)\n\u001b[32m   2542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:2537\u001b[39m, in \u001b[36mvectorize._call_as_normal\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2534\u001b[39m     vargs = [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[32m   2535\u001b[39m     vargs.extend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[32m-> \u001b[39m\u001b[32m2537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:2618\u001b[39m, in \u001b[36mvectorize._vectorize_call\u001b[39m\u001b[34m(self, func, args)\u001b[39m\n\u001b[32m   2616\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Vectorized call to `func` over positional `args`.\"\"\"\u001b[39;00m\n\u001b[32m   2617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vectorize_call_with_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m   2620\u001b[39m     res = func()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:2656\u001b[39m, in \u001b[36mvectorize._vectorize_call_with_signature\u001b[39m\u001b[34m(self, func, args)\u001b[39m\n\u001b[32m   2653\u001b[39m nout = \u001b[38;5;28mlen\u001b[39m(output_core_dims)\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m np.ndindex(*broadcast_shape):\n\u001b[32m-> \u001b[39m\u001b[32m2656\u001b[39m     results = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2658\u001b[39m     n_results = \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m   2660\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nout != n_results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/desktop/music-recommender/.venv/lib/python3.11/site-packages/librosa/sequence.py:1303\u001b[39m, in \u001b[36mviterbi.<locals>._helper\u001b[39m\u001b[34m(lp)\u001b[39m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_helper\u001b[39m(lp):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;66;03m# Transpose input\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1303\u001b[39m     _state, logp = \u001b[43m_viterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_p_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1304\u001b[39m     \u001b[38;5;66;03m# Transpose outputs for return\u001b[39;00m\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _state.T, logp\n",
      "\u001b[31mSystemError\u001b[39m: CPUDispatcher(<function _viterbi at 0x72ab9cf49ee0>) returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "# Basic usage - extract features and train a model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Extraction pipeline\n",
    "extraction_pipeline = Pipeline([\n",
    "    ('spectrogram', SpectrogramExtractor(\n",
    "        new_sr=16000,\n",
    "        save_cache=True\n",
    "    )),\n",
    "    ('features', StatsFeatureExtractor(\n",
    "        sr=16000,\n",
    "        enable_cache=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Extract features\n",
    "X_train_extracted = extraction_pipeline.fit_transform(X_train)\n",
    "X_test_extracted = extraction_pipeline.transform(X_test)\n",
    "\n",
    "# Train model on extracted features\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train_extracted, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_extracted)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"R²: {r2_score(y_test, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
